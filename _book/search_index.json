[["index.html", "portfolio Arthur Timmermans 1 prerequisites", " portfolio Arthur Timmermans Arthur Timmermans 1 prerequisites Welcome to my personal portfolio. On this website I will give more inside into what my skills are and which projects I work(ed) on. I am a life science student at the Hoge School Utrecht (HU), and will graduate in 2023 as a lab technician. Within the science field, not just for data analysiss, but also for science in a whole, I strongly believe in the movement towards open science. In my personal opinion, being open about our research and sharing our knowledge and experience will help to reunite people. And when we work together, feats that seem impossible right now suddenly become achievable. While making this website, I worked in a github repository in RMD files. To see the underlying code of my website, you can visit my github reposetory. Make sure to read the readme on the home page for further instructions on how to use my github repository. For those who are interested in getting more underlying information about me, here is my resume: resume "],["loading-sorting-and-visualising-data.html", "2 loading, sorting and visualising data 2.1 analysing the data:", " 2 loading, sorting and visualising data For both readability and data analysis is it important to keep data tidy. Especially when loading data in, it is important to check if the format is tidy for the data analysis that will be conducted. In this chapter, I will show an example of how I loaded in an excel document, selected the needed data and made the used tables tidy. The excel document contains data from an experiment conducted by by J. Louter (INT/ILC).In this experiment, the influence of 3 substances on the growth of C. Elegans is tested for their toxicity.The more growth is inhibited, the more toxic the substance is expected to be. The substances that were tested are: 2,6 disopropylnaphealene decane naphthalene Further more, there is a positive and negative control. The positive control for this experiments is incubated in ethanol, which is expected to be very toxic and inhibit the growth of C. elegans a lot. The negative control for this experiment is incubated in s-medium, which is expected to offer proper living conditions and allow C. elegans to grow and live. The raw data is shown in figure 2.1. Within this table: The expType column shows if the data is a experimental condition or a control The rawdata column shows the living C. elegans. The compName Column shows the compound name The compConcentration column shows the compound concentration. More metadata is available in the original Excel sheet, which can be found here. However, in figure 2.1 only the main variables that will be worked with are shown for readability. Figure 2.1: table with raw data from c. elegans project based upon the names and values, as shown in figure 2.1, the expected data-types are: expType: character Raw-data: numeric compNames: character compConcentration: numeric In figure 2.1, Only the raw-data is numeric, both the compNames, compConcentrations and expType are character strings. This would suggest that something went wrong during importation of the samples. This is also seen in figure 2.2, where the data was not sorted based upon the actual size, because it is being read as character and therefor is literally sorted based upon the characters.This is an error that occurs more often when importing files into a R object, and showcases why it is important to check the datatypes after importing data. Figure 2.2: graph without changing data type When correcting the data type of the compound concentration back to numeric, the graph in figure 2.3 can be created. In this graph, the X-axis shows the compound concentration in a log value. A log value was chosen because on a normal scale, the spread was very difficult to see. The Y axis shows the amount of living C. elegans. On the left side of figure 2.3, the negative control in s medium grew properly. This showcases that the C. elegans nematodes grew as expected. The positive control with ethanol also seems to show a negative correlation between the concentration of ethanol and the living C. elegans. This would showcase that the principal to test the toxicity by correlating concentration of a compound to the amount of living C. elegans worked in this assay. The other compounds also seem inhibit the growth of C. elegans as the compound concentration increases. Figure 2.3: graph with changing data type In this 2.3, the data on the Y axis is still rather spread out. This can be corrected for by using the negative control as a scale of 1 on the Y axis. This brings the data in the plot relatively closer together, making it easier to compare data of different groups too the negative control. The graph would than look like shown in figure 2.4. In this plot the negative correlation between living C. elegans and the increase in compound concentration again seems to become visible. However, because no statistical tests are run yet, the correlation can not officially be claimed to be present yet. Figure 2.4: graph with normalized scale 2.1 analysing the data: to further determine the (toxic) effect of the added substances, a dose-response analysis can be run. In this analysis, a log-logistic model can be used, such as the glm function of R. In this model, each compound would be tested separately and the the amount of living C. elegans and the compound concentration would be used as input. the model would return data including: estimated maximal estimated minimal The output data from the model can be used in the dose.p function from the MASS library. This function will calculate any fractional dosage value, including the the IC50 concentration Now the slope at IC50 can also be calculated, and put into a dot-plot with a trend line. The different graphs from different compounds can be compared to the positive and negative control and to each other, to estimate if there is a difference in toxicity. If a difference is suspected, the calculated values, including the maximal, minimal and IC 50 can be compared. This comparison can be done by: testing for normality: levenes test for normality if normaly distrubuted: unpaired t.test if not normally distributed: Mann-Whitney U-test compare found P-values. However, as analysis the entire data set is not the goal of this chapter, the further data analysis will not be conducted. "],["reproducibility.html", "3 reproducibility 3.1 paper1 3.2 paper2", " 3 reproducibility Making sure that research is reproducible, is an important measure to avoid fraud and generating results based on the general interest of a person or company. A big step towards making research reproducible is the movement towards open science, where the methods of both research and the data analysis are shared. In my opinion, open research helps to speed up progression and holds potential to decrease fraud. Therefor, I support the movement towards open science. In this chapter, I will review 2 papers, to show that I am able to determine how open the researchers are about their methods for both the laboratory work and the data analysis. The first paper will be a more traditional research paper, and is found on pubmed. This paper will be reviewed based upon following Repita criteria, as shown in figure 3.1. The second paper, will be a r code paper, and is found on the OSF website. The review of this paper will focus more heavily on the readability and reproducibility of the r code and will be less centered around the criteria shown in figure 3.1 Figure 3.1: table with repita criteria for reviewing articles 3.1 paper1 The following article will be reviewed: (ahmedFivedayCourseIvermectin2021?) this study involved 72 patients from Dhaka, Bangladesh (criteria were age 1865 years;admitted to hospital within the last 7 days; presence of a fever (37.5 °C), cough, and/or sore throat; diagnosed positive for SARS-CoV-2) the patients were seperated into 3 groups: - oral ivermectin alone (12 mg once daily for 5 days) - oral ivermectin in combination with doxycycline (12 mg ivermectin single dose and 200 mg doxycycline on day 1, followed by 100 mg every 12 h for the next 4 days) - a placebo control group. The primary endpoints that were measured were the time required for virological clearance, and the remission of fever (37.5 °C) and cough within 7 days. in conclusion, the study found that Virological clearance was earlier in the 5-day ivermectin treatment arm when compared to the placebo group (9.7 days vs 12.7 days; p = 0.02). However, the virological clearance was not lower for the ivermectin + doxycycline arm (11.5 days; p = 0.27). There were no severe adverse drug events recorded in the study. A 5-day course of ivermectin was found to be safe and effective in treating adult patients with mild COVID-19. Larger trials, however, would still be needed to confirm these preliminary findings. 3.1.1 study purpose: The study purpose was clearly described in the introduction. Locating the purpose was easy as well. The authors/article did a good job at this. The purpose was: The study was performed to evaluate the rapidity of viral clearance and safety of a 5-day course of ivermectin or a single-dose of ivermectin and a 5-day course of doxycycline in the treatment of mild COVID-19 in adults. 3.1.2 data and code availibality/location: there was no data availability statement found, neither was the (raw) data found anywhere linked in the article. The code was also never mentioned in the article. This is not uncommon for these kinds of articles, however, it makes the article less reproducible and puts it out of contention for the movement of open science. 3.1.3 study location: the study location, and origin of the patients has been given. The criteria which had to be met for patients to take place in the research has also been listed. I provided these in the introduction above. Even though the data itself was not provided, tracing the locations of the study is now still possible which is a proper move towards open science. 3.1.4 author review: proper information of how to contact the authors has been provided in the paper, mail adresses of all authors were available. The professionalism of the authors could also be verified. In my opinion, the authors seemed professional and this makes the findings of the article more reliable. 3.1.5 ethics statement: there is an ethics statement in the study, for approval of the experimental setup by both medical institutes and the patients. However, no further mention has been made of ethics regarding the experimental data. 3.1.6 funding statement: A funding statement was provided, the study was funded by Beximco Pharmaceutical Limited, Bangladesh. 3.1.7 conclusion: in conclusion: this paper has properly described their findings, sources and conclusions.Somethings like the professionalism of the authors and the funding/interest of the project can be verified. However, the raw/processed data is not available nor is the code with which the data was analysed. Therefor, this paper is not entirely open science and can still improve, by providing the data and the used code to analyse the data. 3.2 paper2 the second article that will be reviewed is: (harveyWhoDeliversVotes2022?) multiparty elections provide a variety of benefits for regime survival. Specifically, the introduction of local elections in otherwise closed regimes has been shown to generate improvements in government legitimacy and performance. However, some non-democratic governments do not hold local executive elections, with the hopes of increasing control over local political resources. In this paper, is tested if appointed local executives in Russian voting will produce more election manipulation for national parties and if this comes at a cost to the partys un-manipulated support. To test this theory, the study will use an election-forensic analysis of precinct-level election data from 176 Russian cities over six national elections from 2003 to 2012. this paper proposes that appointed local executives will produce more election manipulation for national parties, but that this comes at a cost to the partys un-manipulated support. 3.2.1 analysing the code the provided r code appears to have a few goals: loading in data making the data tidy (removing NA, proper row/column organisation for the data analysis, etc.) visualizing and interpreting the data creating marginal structural models to account for selection bias and confounding. run the model and put the results into plots The code has a good readability. It is properly spaced out, and lines of code are provided with comments to explain the code. However, not all code lines have comments. This makes some of the code, sightly harder to read. Therefor, in terms of readability, I rate the code of this paper a 4 out of 5. After downloading and running the code, I hardly had to make any changes at all to let the code run. The only changes that were required, were: installing the packages I didnt have yet with the install.package() function. specify where I stored the data files on my computer With the provided code, I was also easily able to recreate the images that were in the original paper. An example of this, is shown in figure 3.2. This was figure2 in the original paper. In this figure, it is visible that Putin has a far higher manipulation than the other politicians and that appointed exec party also has sightly higher manipulation. Because it was so easy to run the code, and recreate the same figures, I would rate the reproducibility of this code a 5 out of 5. The used code is further shown down bellow figure 3.2. Figure 3.2: recreated graph from reviewed paper 3.2.2 raw code of paper 2 library(tidyverse) library(interactions) library(stargazer) library(fastDummies) ##################################################################################################################### # this is the only section that requires changes, add your own location where you saved the .csv files. full.data &lt;- read.csv(&quot;raw/coefs with city covariates cleaned.csv&quot;) precincts_total &lt;- read.csv(&quot;raw/n_precincts_all.csv&quot;) ##################################################################################################################### full.data &lt;- full.data %&gt;% mutate(presidential = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, 1, 0)) full.data &lt;- full.data %&gt;% mutate(known.opp.mayor = ifelse(is.na(opp_mayor1) == T, 0, ifelse(opp_mayor1==1, 1,0))) full.data &lt;- full.data %&gt;% mutate(appointed.exec = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x &gt;= cancel.year, 1, 0))) full.data &lt;- full.data %&gt;% dplyr::rename(turnout.coef = turnout) full.data &lt;- full.data %&gt;% mutate(years.with.elections = ifelse(is.na(cancel.year)==T, year.x-1996, ifelse(year.x - cancel.year &lt; 0, year.x - 1996, cancel.year - 1996))) #This gives a measure of early vs late vs. control; 1996 bc this is when local elections first took place full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x - cancel.year &lt; 0, 0, year.x - cancel.year))) full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2007 &amp; year.x ==2007, 1, years.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2011 &amp; year.x ==2011, 1, years.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x - cancel.year &lt; 0, 0, 1))) full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2007 &amp; year.x ==2007, 1, treated.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2011 &amp; year.x ==2011, 1, treated.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(years.under.appt = ifelse(is.na(cancel.year) == T, 0, ifelse(year.x - cancel.year &lt; 0, 0, ifelse(year.x == 2007 | year.x == 2011 &amp; year.x == cancel.year, 1, year.x - cancel.year)))) full.data &lt;- full.data %&gt;% mutate(city_id_factor = as.factor(city_id)) full.data &lt;- full.data %&gt;% mutate(treatment.group = ifelse(is.na(cancel.year)==F, 1, 0)) full.data &lt;- full.data %&gt;% mutate(control = ifelse(is.na(cancel.year)==T, 1, 0)) full.data &lt;- full.data %&gt;% mutate(early.treated = ifelse(is.na(cancel.year)==F &amp; cancel.year &lt;= 2007, 1, 0)) full.data &lt;- full.data %&gt;% mutate(late.treated = ifelse(is.na(cancel.year)==F &amp; cancel.year &gt; 2007, 1, 0)) full.data &lt;- full.data %&gt;% mutate(years.under.regime = ifelse(appointed.exec == 0, year.x - 2000, year.x - cancel.year)) ##Dummies for ethnic categorical full.data &lt;- dummy_cols(full.data, select_columns = &quot;ethnic.categorical&quot;) full.data &lt;- full.data %&gt;% dplyr::rename(majority.russian.republic = ethnic.categorical_Majority_Russian_republic) full.data &lt;- full.data %&gt;% dplyr::rename(majority.minority.republic = `ethnic.categorical_Majority-minority_republic`) ##Adding n_precincts precincts_total &lt;- precincts_total %&gt;% mutate(city_id_year = paste(city_id, year, sep = &quot;_&quot;)) precincts_total &lt;- precincts_total %&gt;% dplyr::select(-year, -city_id) full.data &lt;- left_join(full.data, precincts_total, by =&quot;city_id_year&quot;) full.data &lt;- full.data %&gt;% dplyr::rename(n.precincts = n) ##Share of appointed mayors nationwide full.data &lt;- full.data %&gt;% group_by(year.x) %&gt;% mutate(total.appointed = sum(appointed.exec, na.rm=T)) full.data &lt;- full.data %&gt;% group_by(year.x) %&gt;% mutate(total.cities = length(unique(city_id))) full.data &lt;- full.data %&gt;% mutate(appointed.share = total.appointed/total.cities) ## Creating &#39;.mod&#39; variables, which are lagged one year for presidential election years full.data &lt;- full.data %&gt;% mutate(urgov.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(urgov, 1), urgov)) full.data &lt;- full.data %&gt;% mutate(mayor.tenure.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(mayor.tenure, 1), mayor.tenure)) full.data &lt;- full.data %&gt;% mutate(lnAvgSalary.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(lnAvgSalary, 1), lnAvgSalary)) full.data &lt;- full.data %&gt;% mutate(dem.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(dem, 1), dem)) full.data &lt;- full.data %&gt;% mutate(comp.scale.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(comp.scale2, 1), comp.scale2)) 3.2.2.1 Overall manipulation model.tc2 &lt;- lm(turnout.coef ~ factor(city_id) + factor(year.x) + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + appointed.exec, data = full.data) #summary(model.tc2) #sjPlot::plot_model(model.tc2, type = &quot;int&quot;, show.data = T) #sim_slopes(model.tc2, pred = appointed.exec, modx = years.under.regime, jnplot = TRUE) model.tc.base &lt;- lm(turnout.coef ~ factor(city_id) + presidential + xconst.lag + putin.app.3mo + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + appointed.exec, data = full.data) #summary(model.tc.base) ##Plot base model p.tc.base &lt;- sjPlot::plot_model(model.tc.base, colors = &quot;bw&quot;, terms = c(&quot;presidential&quot;, &quot;xconst.lag&quot;, &quot;putin.app.3mo&quot;, &quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;)) + ylim(-.45, 1.65) + labs(title = &quot;Overall manipulation&quot;) + theme_bw() + geom_hline(aes(yintercept = 0)) p.tc.base &lt;- p.tc.base + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;, &quot;Putin approval&quot;, &quot;Exec. constraints&quot;, &quot;Presidential election&quot;)) ##Plot second model p.tc.2 &lt;- sjPlot::plot_model(model.tc2, colors = &quot;bw&quot;, terms = c(&quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;)) + ylim(-.45, .45) + labs(title = &quot;Overall manipulation&quot;) + theme_bw() + geom_hline(aes(yintercept = 0)) p.tc.2 &lt;- p.tc.2 + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;)) #Write the coefficient names in backwards order, because the plot is transposed 3.2.2.2 UR voteshare model.ur2 &lt;- lm(ur.voteshare ~ factor(city_id) + factor(year.x) + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + turnout.coef + appointed.exec, data = full.data) #summary(model.ur2) #sjPlot::plot_model(model.ur2, type = &quot;int&quot;) model.ur.base &lt;- lm(ur.voteshare ~ factor(city_id) + presidential + xconst.lag + putin.app.3mo + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + turnout.coef + appointed.exec, data = full.data) #summary(model.ur.base) p.ur.base &lt;- sjPlot::plot_model(model.ur.base, colors = &quot;bw&quot;, terms = c(&quot;presidential&quot;, &quot;xconst.lag&quot;, &quot;putin.app.3mo&quot;, &quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;years.under.regime&quot;, &quot;turnout.coef&quot;, &quot;appointed.exec&quot;)) + ylim(-.25, .5) + labs(title = &quot;United Russia vote-share&quot;) + geom_hline(yintercept=0) + theme_bw() p.ur.base &lt;- p.ur.base + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Turnout coef.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;, &quot;Putin approval&quot;, &quot;Exec. constraints&quot;, &quot;Presidential election&quot;)) p.ur.2 &lt;- sjPlot::plot_model(model.ur2, colors = &quot;bw&quot;, terms = c(&quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;, &quot;turnout.coef&quot;)) + ylim(-.1, .5) + labs(title = &quot;United Russia vote-share&quot;) + geom_hline(yintercept=0) + theme_bw() p.ur.2 &lt;- p.ur.2 + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Turnout coef.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;)) 3.2.2.3 Table stargazer(model.tc.base, model.tc2, model.ur.base, model.ur2, type = &quot;html&quot;, digits = 2, omit = &quot;factor&quot;, out = &quot;table_main.html&quot;) 3.2.2.4 Multiplot # Multiple plot function # # ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects) # - cols: Number of columns in layout # - layout: A matrix specifying the layout. If present, &#39;cols&#39; is ignored. # # If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), # then plot 1 will go in the upper left, 2 will go in the upper right, and # 3 will go all the way across the bottom. # multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } 3.2.2.5 Plotting png(filename = &quot;coefplot manipulation multiple.png&quot;, width = 7, height = 5, res = 500, units = &quot;in&quot;) multiplot(p.tc.base, p.tc.2, cols = 1) dev.off() png(filename = &quot;coefplot voteshare multiple.png&quot;, width = 7, height = 5, res = 500, units = &quot;in&quot;) multiplot(p.ur.base, p.ur.2, cols = 1) dev.off() "],["ordering-datafiles.html", "4 ordering datafiles 4.1 basic frame work 4.2 version control", " 4 ordering datafiles Besides keeping a data file tidy, it is also important to keep the folders in which these data files are stored tidy. A properly made directory with version control, helps to avoid that data gets lost. It also generally improves speed and reproducibility, especially when working in a team. 4.1 basic frame work To store, edit, analyse and maintain my data, I follow the basic Guerrilla analytics framework, written by Enda Ridge. The basic frame work is: Space is cheap, confusion is expensive Use simple, visual project structures and conventions Automate with program code Link stored data to data in the analytics environment to data in work products (rmarkdown) Version control changes to data and analytics code (Git/Github.com) Consolidate team knowledge (agree on guidelines and stick to it as a team) Use code that runs from start to finish The full description of Guarilla analytics framework can be found here, and it is recommendable to read through this for anyone who is not yet familiar with the framework. There are of course still plenty of other proper frame works designed for working tidy and reproducible. The key is in remaining consistent and sticking with something that works well for you. Therefor I also encourage doing your own research and finding a method that works well for you and your team. 4.1.1 directory tree of this project In the directory tree bellow, I showed the directory tree from this portfolio, as an example of how I think data can be stored tidy. A few notes of thing I try to keep track of: I avoid deep bedding (max 2-3 folder layers) to keep the relative path short Every folder is given a readme.txt file, giving a description of all files/folders that are present, the origin of the files, and the date at which the files are downloaded The name structure of the files is consistent and does not contain special caracters (accept for _ and . ) ## ~/documents/hu/R/DSFB2/portofolio ## +-- 01_portofolio_opdracht1.1.rmd ## +-- 02_portofolio_opdracht1.2.rmd ## +-- 03_portofolio_opdracht2.Rmd ## +-- 04_portofolio_opdracht3.2.Rmd ## +-- 06_portfokio_opdracht5.html ## +-- 06_portfokio_opdracht5.Rmd ## +-- 07_portfolio_opdracht7.Rmd ## +-- 08_portfolio_opdracht8.Rmd ## +-- 09_portfolio_opdracht9.Rmd ## +-- 10_refrences.Rmd ## +-- bibliography.bib ## +-- CV_files ## | +-- font-awesome-5.1.0 ## | | +-- css ## | | | +-- all.css ## | | | \\-- v4-shims.css ## | | \\-- webfonts ## | | +-- fa-brands-400.eot ## | | +-- fa-brands-400.svg ## | | +-- fa-brands-400.ttf ## | | +-- fa-brands-400.woff ## | | +-- fa-brands-400.woff2 ## | | +-- fa-regular-400.eot ## | | +-- fa-regular-400.svg ## | | +-- fa-regular-400.ttf ## | | +-- fa-regular-400.woff ## | | +-- fa-regular-400.woff2 ## | | +-- fa-solid-900.eot ## | | +-- fa-solid-900.svg ## | | +-- fa-solid-900.ttf ## | | +-- fa-solid-900.woff ## | | \\-- fa-solid-900.woff2 ## | \\-- paged-0.18 ## | +-- css ## | | \\-- resume.css ## | \\-- js ## | +-- config.js ## | +-- hooks.js ## | \\-- paged.js ## +-- data ## | +-- dengue_tidy.csv ## | +-- dengue_tidy.rds ## | +-- flu_tidy.csv ## | +-- flu_tidy.rds ## | +-- gapminder.csv ## | +-- gapminder.rds ## | \\-- readme ## +-- html ## | +-- cv.html ## | +-- ols models.nb.html ## | +-- readme.txt ## | \\-- table_main.html ## +-- images ## | +-- 09_plot1.png ## | +-- 09_plot2.png ## | +-- 09_plot3.png ## | +-- coefplot_manipution_multiple.png ## | +-- cv.png ## | \\-- readme.txt ## +-- index.Rmd ## +-- portofolio.Rproj ## +-- python scripts ## | +-- calculator.py ## | +-- jupyter_notebook ## | | +-- drug200.csv ## | | +-- machine_learning_with_python.ipynb ## | | \\-- metadata.xlsx ## | +-- password-generator.py ## | +-- readme ## | +-- reminder-to-get-up.py ## | \\-- rock-paper-scisorcs.py ## +-- r ## | +-- installing_packages.R ## | +-- readme.txt ## | \\-- set_up_script.R ## +-- raw ## | +-- 007_dengue.csv ## | +-- 007_flu.csv ## | +-- CE.LIQ.FLOW.062_Tidydata.xlsx ## | +-- coefs with city covariates cleaned.csv ## | +-- data_covid19_cases.csv ## | +-- n_precincts_all.csv ## | +-- readme.txt ## | \\-- table_assignment1.2.xlsx ## +-- README.md ## +-- rmd ## | +-- cv.Rmd ## | \\-- ols models.Rmd ## +-- _book ## | +-- 404.html ## | +-- aside.html ## | +-- free-studying-period.html ## | +-- images ## | | +-- 09_plot1 - Copy.png ## | | +-- 09_plot1.png ## | | +-- 09_plot2 - Copy.png ## | | +-- 09_plot2.png ## | | +-- 09_plot3.png ## | | +-- coefplot_manipution_multiple.png ## | | \\-- cv.png ## | +-- index.html ## | +-- libs ## | | +-- anchor-sections-1.1.0 ## | | | +-- anchor-sections-hash.css ## | | | +-- anchor-sections.css ## | | | \\-- anchor-sections.js ## | | +-- crosstalk-1.2.0 ## | | | +-- css ## | | | | \\-- crosstalk.min.css ## | | | \\-- js ## | | | \\-- crosstalk.min.js ## | | +-- datatables-binding-0.23 ## | | | \\-- datatables.js ## | | +-- datatables-css-0.0.0 ## | | | \\-- datatables-crosstalk.css ## | | +-- dt-core-1.11.3 ## | | | +-- css ## | | | | +-- jquery.dataTables.extra.css ## | | | | \\-- jquery.dataTables.min.css ## | | | \\-- js ## | | | \\-- jquery.dataTables.min.js ## | | +-- gitbook-2.6.7 ## | | | +-- css ## | | | | +-- fontawesome ## | | | | | \\-- fontawesome-webfont.ttf ## | | | | +-- plugin-bookdown.css ## | | | | +-- plugin-clipboard.css ## | | | | +-- plugin-fontsettings.css ## | | | | +-- plugin-highlight.css ## | | | | +-- plugin-search.css ## | | | | +-- plugin-table.css ## | | | | \\-- style.css ## | | | \\-- js ## | | | +-- app.min.js ## | | | +-- clipboard.min.js ## | | | +-- jquery.highlight.js ## | | | +-- plugin-bookdown.js ## | | | +-- plugin-clipboard.js ## | | | +-- plugin-fontsettings.js ## | | | +-- plugin-search.js ## | | | \\-- plugin-sharing.js ## | | +-- htmlwidgets-1.5.4 ## | | | \\-- htmlwidgets.js ## | | \\-- jquery-3.6.0 ## | | \\-- jquery-3.6.0.min.js ## | +-- loading-sorting-and-visualising-data.html ## | +-- main.html ## | +-- ordering-datafiles.html ## | +-- parameterization.html ## | +-- project-liquid-biopsies.html ## | +-- refrences-1.html ## | +-- refrences.html ## | +-- reproducibility.html ## | +-- rpackages.html ## | +-- search_index.json ## | +-- structured-query-language.html ## | \\-- _main_files ## | \\-- figure-html ## | +-- 01graph1-1.png ## | +-- 01graph2-1.png ## | +-- 01graph3-1.png ## | +-- 07graph1-1.png ## | +-- 07graph2-1.png ## | +-- 07graph3-1.png ## | +-- 07graph4-1.png ## | +-- 07graph5-1.png ## | +-- 07graph6-1.png ## | +-- flu_graph1-1.png ## | +-- graph1-1.png ## | +-- graph2-1.png ## | +-- graph3-1.png ## | +-- showing correct grap c elegans-1.png ## | +-- showing wrong graph c elegans-1.png ## | +-- standard dengue graph-1.png ## | +-- standard flu graph-1.png ## | \\-- together-1.png ## +-- _bookdown_files ## +-- _main.Rmd ## \\-- _main_files ## \\-- figure-html ## +-- 01graph1-1.png ## +-- 01graph2-1.png ## +-- 01graph3-1.png ## +-- 07graph1-1.png ## +-- 07graph2-1.png ## +-- 07graph3-1.png ## +-- 07graph4-1.png ## +-- 07graph5-1.png ## +-- 07graph6-1.png ## +-- flu_graph1-1.png ## +-- graph1-1.png ## +-- graph2-1.png ## +-- graph3-1.png ## +-- showing correct grap c elegans-1.png ## +-- showing wrong graph c elegans-1.png ## +-- standard dengue graph-1.png ## +-- standard flu graph-1.png ## \\-- together-1.png 4.1.2 directory tree of another project Another example of how a directory tree can look is shown in figure bellow. This is one of my directories of a previous school project. I recently made the format of this project tidy according to the principals mentioned above. ## ~/documents/hu/R/DSFB2/daur2 ## +-- eindopdracht ## | +-- html ## | | \\-- eindopdracht.html ## | +-- images ## | | +-- image1_summaries_fastqc_analysis.png ## | | +-- image2_per_base_sequence_quality_fastqc_analysis.png ## | | \\-- image3_per_sequence_quality_fastqc_analysis.png ## | +-- raw ## | | \\-- identifyer.txt ## | +-- readme.txt ## | +-- rmd ## | | \\-- eindopdracht.Rmd ## | \\-- scripten ## | \\-- set_up.R ## +-- meta_genomics ## | +-- html ## | | +-- formatieve_opdracht.html ## | | \\-- les_opdrachten.html ## | +-- raw ## | +-- readme.txt ## | +-- rmd ## | | +-- formatieve_opdracht.Rmd ## | | \\-- les_opdrachten.Rmd ## | \\-- scripten ## | \\-- set_up.R ## \\-- rnaseq ## +-- html ## | +-- formatieve_opdracht.html ## | \\-- les_opdrachten.html ## +-- raw ## +-- readme.txt ## +-- rmd ## | +-- formatieve_opdracht.Rmd ## | \\-- les_opdrachten.Rmd ## \\-- scripten ## \\-- set_up.R 4.2 version control besides ordering the data structures on my own computer, I use version control with github for both my own repositories as well as those which I work on in projects. Working with github on group projects has helped me in many ways. It makes it far easier to work on the same documents together. Further more, we can far easier merge files, without the need for many different in between versions of the same file. This improves both the speed, accuracy and communication within our projects. Within my personal repositories github helped to avoid that data would get lost, and made going back to older versions far easier. Further more, switching between different computers is also far easier with github, if the code is set up robust. "],["free-studying-period.html", "5 free studying period 5.1 planning and organisation to learn python 5.2 the basics 5.3 machine learning 5.4 code editors 5.5 conclusion", " 5 free studying period I have taken a course data science during the minor of my study. One of our assignments was to set our own learning goals for skills we want to obtain in the future. During my minor, I spend about 30-40 hours on getting started with learning these new skills. The aim was to get started, and to lay a foundation on which I could build further after I graduated. On this page, I will describe what I choose to do with this free learning period and how I spend my time on it. I got started by asking myself what I wanted to do and Where I wanted to be in ~2 years time. It was during this time that I realized I still have multiple things I want to learn and develop myself in within the life science field. These include: working in an ecological lab clinical lab work (like laboratory work in a hospital) working in a project with organoids aside from these goals within the life science field, I also still had many personal goals. However, because these do not relate to this portfolio I did not add them here. But, it did rather quickly become clear to me that there is still a lot that I want to learn and do.This made it difficult for me to set my sight on a specific goal. Especially since Im not certain if I still want to continue working in the field of bioinformatics and data science in the future. Therefor, I decided to take a more general approach and get a wider few of whats all still possible for me with bioinformatics. This would help me to more easily adept, and learn specific skills later when Ive a more clear vision of what I want to do. To generally widen my view, I decided to learn python. This would be useful to more easily flow into jobs where python is a required programming language. Further more, python is a very useful language for software creation, which I would like to learn, to create my own software at some point in the future. This latter use is not necessarily needed for my job within life sciences, but, certainly could be an useful skill to obtain nevertheless. At the time I decided I wanted to learn how to program in python, I had no experience or knowledge of how to do so. Therefor, I had to start learning this new language from scratch, and start at the basics. 5.1 planning and organisation to learn python When trying to learn a new skill, a proper plan makes the learning process more structured and helps to see if your goals are achieved. In this free period I want to take the time to learn some basic python from some tutorials on YouTube, and after that use this knowledge to create some own scripts, and to learn some basic machine learning. furthermore, I will try to use some different code editors. My general plan is: describing my learning process on this page and showing what I learned (5 hours) get to know the basic code structure of Python (15 hours) watch YouTube tutorials read through some basic python scripts and try to recreate my own apply the gained skills to create my own scripts from scratch machine learning in python (15 hours) watch YouTube tutorials, and write along with the videos to recreate the tutorial scripts apply the gained skills to create my own scripts from scratch and apply them to a dataset getting familiar with other code editors (5 hours): pychar Jupyter Notebook Visual Studio Code obsidian 5.2 the basics to get a general idea of how python works and what all possibilities are, I got started with watching this youtube video. in this video, I learned the following things: Variables Receiving Input Type Conversion Strings Arithmetic Operators Operator Precedence Comparison Operators Logical Operators If Statements While Loops Lists List Methods For Loops The range() Function Tuples This helped me to better understand the basic structure of python. In this learning process I noticed that it was relatively easy to adept to Python, because there are many similarities between Python and R. Therefor, I picked python up rather easily. After watching the mentioned above video, some googling, and playing around with some data I felt ready to start writing my own scripts. I got started with making some basic python scripts, like: a basic calculator rock-paper-scissors game password generator a timer which can remind me to get up while coding all my python scripts can be found in my personal portfolio, which was linked in the introduction. 5.3 machine learning As I was learning the basics of python, I got more interested in machine learning and the possibilities that machine learning brings with it. Therefor, I watched this youtube video to learn more about machine learning (rather interesting I learn how to make a machine learn, aint it?). I got familiar with some of the most famous python machine learning libraries, including: numpy pandas matplotlib scikit-learning I got started with machine learning in pycharm, as this also the environment where I got started with learning python. However, the constant data inspection was rather messy and difficult in the terminal. Therefor, I rather quickly switched to using Jupyter notebook, as frequent data inspection is far easier in this program. The first form of machine learning I practiced with, was with a decision tree algorithm. This is an algorithm that is rather easy to understand and use. I used a decision tree algorithm to determine which drug patients received based on their Age, sex, blood pressure, Cholesterol, and Na_to_K. This is a rather simple method, that falls apart when using bigger more complex datasets. However, it is still sufficient to get some basic understandings. The results of this are shown in the jupyter notebook file located in my personal portfolio. 5.3.1 neuro networks A rather famous kind of machine learning that Ive heard a lot about are neuro networks. I did not understand how they worked, and therefor, I watched these videos: what is a neuronetwork? how neuronetworks learn what does backpropagation do? Backpropagation calculus In these videos, the creator really inspired me and suggested me to check out this book by Michael Nielsen introducing neural networks and deep learning. In this book, neuro networks get explained in more depth and an example with code is given. A neuro network was trained to recognize hand written digits with 96% accuracy.All code and data was open source and accessible from this github page.I also tried to type along to recreate the network. However, for this I was to inexperienced and the network did not work. Therefor, I did not include that into this repository. 5.4 code editors The last half Year, I have used Rstudio, which has worked well for me so far. However, while learning python, I have seen people using many different code editors. And to further widen my few on what is all possible, I decided to try a few other code editors myself. Down bellow I describe which code editors I used and what my overall experience with them was. 5.4.1 pycharm I got started with learning python in PyCharm. This is an Integrated Development Environment (IDE). My overall experience with Pycharm was very positive. In Pycharm it is easy to create and manage projects and the build in vision control mechanisms work well. It is also possible to integrate git into pycharm. Further more The code editing is easy with good auto-indentation. The ease of use helped me to get comfortable with Python quickly. There are also many more advances options, and many customization options with packages, which makes this a great code editor for both new and experienced data annalists. A downside of Pycharm is that that the professional version with all features needs to be purchased. This makes this program less suitable within the idea of reproducible research. Therefor, I decided to look for another (free) code editor or IDE. 5.4.2 visual studio code soon after getting started with watching python tutorials, I saw people using Visual Studio Code (VSC). This is an open-source text editor, and by far the most used code editor. Because this code editor is open-source, it is more suitable for reproducible research. There are also many packages available due to the wide spread use of VSC. I noticed that, compared to pycharm, VSC is a more light weight code editor, that is easier on the computer but offers less processing power in return. The overall inter phase was easy to work with and offered a lot of customization. There is also a git plug in available, to add version control. This made VSC a pleasant code editor to work with and a great alternative to rstudio. 5.4.3 jupyter notebook I started working with jupyter notebook while learning some basics about machine learning. The main benefit of jupyter notebook is that, all results will be stored under the code chunks. These results will be saved and maintained until the code chunk is ran again. Therefor, code chunks only need to be run once and looking back at results becomes far easier. This makes it much easier to make small changes based upon earlier results. And further more, it felt convenient that unlike in R, where the data in the terminal disappears each time the r session is terminated, the data in jupyter notebook was maintained. Just like R studio and cisual studio code, jupyter notebook is free and helps the movement towards open science. 5.4.4 obsidian I came across obsidian while watching this youtube video. Obsidian allows to edit R markdown documents and create networks between markdown files. Because I mainly focused on python In this period, I didnt go to much in depth into how obsidian works. But it seems like a program Id still want to try in the future, to possibly replace Rstudio. 5.5 conclusion As I worked with python in various different types of code editors, and than switched back to R studio to write this markdown file, I noticed how quickly I had grown unfamiliar with rstudio and that I started to confuse shortcuts and features from other programs. This showed me that, even though switching between different langues and programs may be relatively easy, it also has high value to be well familiar with the tools you use. Therefor, my entire idea that knowing a bid about everything, was not correct. And I really noticed that it is better to know 1 or 2 langues and programs well than to know a bunch of them poorly. Therefor, I will for now stick with rstudio and VScode, and if I need to learn any other programs in the future, I will make the switch at the time. "],["project-liquid-biopsies.html", "6 project liquid biopsies 6.1 theoretical background 6.2 R project", " 6 project liquid biopsies Between May and June, 2022, I wrote R-code for the Princess Máxima Center within a project called liquid biopsies. I worked on this project together with 2 other students: Pedro de Bos and Thijmen van Brenk. On this page, I will provide: a theoretical back ground, to highlight the importance of the project liquid biopsies back ground information to what we tried to achieve in our R project a brief summary what the end product of our R project was 6.1 theoretical background Neuroblastoma is an embryonic malignancy that affects normal development of the adrenal medulla and para vertebral sympathetic ganglia in early childhood. Despite extensive studies to the molecular characteristics of human neuroblastomas, the initiation mechanisms and even its origin are still largely unknown. Maris (2010) However, neuroblastoma is the most common extracranial solid tumor in children. Further more, it has high recurrence rates and low survival rates. Therefore, early diagnosis, treatment response evaluation, and recurrence monitoring are of great significance for NB patients.  (2022) A further complication is that neuroblastoma exhibits genetic, morphological and clinical heterogeneity. This in term limits the efficacy of existing treatment. Therefor, Gaining detailed knowledge of the molecular signatures and genetic variations involved in the pathogenesis of neuroblastoma is necessary to develop safer and more effective treatments for neuroblastomas. Zafar et al. (2021) The wide genetic variation, also means that several different kinds of cell progresses can be effected. These include changes in the regulatory role in differentiation, apoptosis, cell proliferation, tumourigenesis, angiogenesis or metastasis of neuroblastoma. The mutations in different cell progresses may require different treatments, which can further complicate treatment when the genetic variation of the tumor cells is not well understood, Aygun (2018) The issues with neuroblastoma is that to properly monitor the progression of the tumor, samples need to be taken often. And tumor biopsies can be very invasive. This may lead to damage from the tumor biopsies, or to a lack of sampling which limits the effectiveness of the treatment. A possible solution, lays with liquid biopsies. Liquid biopsies, is a method of obtaining genomic material of tumors from blood samples. tumors secret DNA to the blood, which is called cell free DNA. This cell free DNA can be amplified, sequences and analysed. A study has shown that the proportion of tumor-derived DNA in cell free DNA was 42.5% (16.9%-55.9% across all samples). This could potentially mean sufficient sensitivity of liquid biopsy for neuroblastoma.@shiraiQuantitativeAssessmentCopy2022 6.2 R project At this moment, data sets of DNA from tumor biopsies and liquid biopsies of patients with neuoblastoma are compared manual at the Princes Maxima Center. This is very time consuming, and when working with time pressure from an increasing number of patients, can lead to human error. And within the view of open science, this method is not reproducible or robust. In our R project, we set out to solve this issue using the R-package called shinny. We used this package to build an interactive web database, which: automates the process of aligning DNA sets of samples allows for easy comparing and visualization of sequencing results introduces a reproducible and tidy work environment gives the possibility to easily save and share files requires a very limited amount of R-knowledge to set up (and comes with a manual) Our working partner at the princess Máxima Center was very satisfied with the end product we delivered. Though, because of an agreement of secrecy, I am not allowed to share any more details about this end product. "],["structured-query-language.html", "7 Structured Query Language 7.1 cleaning data 7.2 setting up a database 7.3 data visualization 7.4 Conclusions on the flu dataset 7.5 Conclusions on the dengue dataset 7.6 joining graphs 7.7 Conclusions after joining", " 7 Structured Query Language Structured Query Language (SQL), is a programming language which was creating for communicating with and editing databases. With the increasing usage of cloud storage and databases, the importance of SQL has become higher as the world got more automated. nearly all databases use SQL as their main programming. Because of the high usage, learning SQL is very relevant. I currently have a basic knowledge of how to use SQL to create a database and to add, edit and remove data from databases. On this page, I will demonstrate what I can all do wit SQL and build a SQL database. For this, I used two files that contained the number of dengue fever cases and flu cases. These files provide cases from 2002 till 2015. The general steps I took in this analysis are: Manual download of the Dengue Fever data set from the dsfb2 repository Manual download of the Flu Fever data set from the dsfb2 repository. Clean and tidy the data Write the data away in the folder data Make a SQL database of the data using RPostgreSQL Data visualization Conclusion 7.1 cleaning data After loading in the data, I inspected the data. The data from the gapminder dataset is shown in table 7.1 The data from the dengue data set is shown in table 7.2 The data from the flu data set is shown in table 7.3 Figure 7.1: data from gap minder dataset Figure 7.2: data from dengue dataset Figure 7.3: data from flu dataset The gapminder data set was already in tidy data format, and therefor did not need to be edited. The dengue and flu data set did require editing to make the tables tidy. To make later comparisons of the different data sets easier, the format of the flu and dengue data sets were changed to match the format of the gapminder data set. The changes made were: Put all countries in one column, and the cases to one column split the column Date to three columns named Year, Month and Day correct data classes The new tidy dengue table is shown in table 7.4 The new tidy flu table is shown in table 7.5 The new tables were stored as a .csv file and as a .rds file in the data folder Figure 7.4: tidy data from dengue dataset Figure 7.5: tidy data from flu dataset 7.2 setting up a database 7.2.1 exporting data to DBeaver the new tidy data files were exporting to DBeaver using Rpostgres: # the link to exporting files to DBeaver requires to fill in personal information such as a password, which is unique for all users. # this analysis is reproducible if you copy the code and insert your own local information. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;, user = &quot;postgres&quot;, password = &quot;fill in your own password here&quot;) dbWriteTable(con, &quot;flu_tidy&quot;, flu_tidy) dbWriteTable(con, &quot;dengue_tidy&quot;, dengue_tidy) dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbDisconnect(con) 7.2.2 SQL script In D beaver, the following script is written to inspect the data SELECT dengue_cases, country FROM dengue_tidy order BY dengue_cases asc SELECT dengue_cases, country FROM dengue_tidy order BY dengue_cases desc SELECT flu_cases, country FROM flu_tidy order BY flu_cases asc SELECT flu_cases, country FROM flu_tidy order BY flu_cases desc SELECT population, country FROM gapminder order BY population asc SELECT population, country FROM gapminder order BY population desc After inspected the data in Dbeaver, the data was also still inspected in R using the dplyr package. Based on these 2 checks, it was concluded that the data was loaded in properly. ## # A tibble: 6,590 x 5 ## Year Month Day Country dengue_cases ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Bolivia 0.101 ## 3 2002 12 29 Brazil 0.073 ## 4 2002 12 29 India 0.062 ## 5 2002 12 29 Indonesia 0.101 ## 6 2002 12 29 Mexico NA ## 7 2002 12 29 Philippines NA ## 8 2002 12 29 Singapore 0.059 ## 9 2002 12 29 Thailand NA ## 10 2002 12 29 Venezuela NA ## # ... with 6,580 more rows 7.2.3 importing data from DBeaver after exporting the data from Rstudio to DBeaver, the data was imported back from DBeaver to Rstudio with the following script: # import the data into R after inspecting the data in DBeaver gapminder &lt;- dbReadTable(con, &quot;gapminder&quot;) dengue &lt;- dbReadTable(con, &quot;dengue_tidy&quot;) flu &lt;- dbReadTable(con, &quot;flu_tidy&quot;) 7.3 data visualization The data will now be visualized to check if the flu or dengue fever is more prevalent in certain areas. The data from the flu dataset is shown in figure 7.6 The data from the dengue dataset is shown in figure 7.7 Figure 7.6: first graph from flu dataset, sorted by country Figure 7.7: first graph from dengue dataset, sorted by country 7.4 Conclusions on the flu dataset When considering the number of flu cases globally as shown in figure 7.6, it becomes visible that the highest occurrence of the flu is in Spain, followed by Canada and the United States. The lowest flu occurrences on the other hand are found in Sweden, followed by Chile and New Zealand. The reason for this is partciular spread of flu cases is unknown. To establish significance its possible to still run additional statistical testing, and to possibly research more metadata. 7.5 Conclusions on the dengue dataset When looking at figure 7.7, its visible that the highest number of dengue cases are seen in Venezuele, followed by Indonesia and Brazil. Dengue fever develops after infection with the Flaviviridae virus carried by mosquitoes. These mosquitoes are more prevalent in countries with warmer climates, which might explain the spread of occurrences that is vissible within the graph. 7.6 joining graphs To be able to better compare the two occurances of flu and dengue fever, the 2 plots will be joined together as is shown in figure 7.8 Figure 7.8: flu and dengue fever graphs joined together 7.7 Conclusions after joining Joining the data set provides us with a singular object that contains data on all three diseases. This helps put things in perspective, however in figure 7.6, 7.7 and 7.8, the conclusions previously drawn remains the same. "],["rpackages.html", "8 rpackages", " 8 rpackages a rather big part of R, are the R packages that are available.These packages can add in many new features and customization. Further more, R packages make sharing code a lot easier. This supports the movement towards open source code and can save up a lot of time for everyone. To demonstrate I know how to build an R package, I created my own custom package which adds a black and white theme to all graphs and plots. My R package, and all instructions on how to use it can be found at: https://github.com/Arthur1Timmermans/fiftyshadesofgrey "],["parameterization.html", "9 parameterization", " 9 parameterization parameterization is a good way to make code more robust and flexible. Furthermore, it also makes it easier for other people to change each other their codes without the risk of adding inconsistencies. In this chapter, I will make a paramererized report of the European Center for Disease Control (ECDC) COVID-19 case data. This data is available at the ECDC website. The aim of this analysis is to create 3 graphs, in which the amount of infections (new cases) and deaths will be mapped by the following params: The country to which the report applies to The year that the reported data applies to The period in months that the report applies to The further aim of this report is to analyse the correlation between the time of the year, the Covid cases and the Covid deaths. At first, I mapped the new cases, which can be seen in plot 1. In this graph, the y axis shows the time in months and the x axis shows the amount of new covid cases. It is visible that by far the largest peak in covid19 cases is measured between October and December in 2021. This would correspond with the flu/cold season. plot1, covid 19 cases in Austria in 2021 The deaths caused by covid19 are mapped in plot2. In this graph, the y axis shows the time in months and the x axis shows the amount of people who died as a result of covid19. Within this plot, a line somewhat similar to the covid19 cases (plot 1) can be seen, with a large spike in covid19 deaths between October and December 2021. To further visualize the simalarity between plot 1 and 2, plot 1 and 2 are also combined using the plot_grind function. This created plot 3, in which a clear connection between the 2 plots can be seen. plot2, covid19 deaths in Austria in 2021 plot3, covid19 cases and deaths in Austria in 2021 "],["refrences.html", "10 refrences", " 10 refrences The links to all used sources: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
