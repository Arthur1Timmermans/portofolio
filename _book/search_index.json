[["index.html", "portfolio Arthur Timmermans 1 prerequisites", " portfolio Arthur Timmermans Arthur Timmermans 1 prerequisites Welcome to my personal portfolio. On this website I will give more inside into what my skills are and which projects I work(ed) on. I am a life science student at the Hoge School Utrecht (HU), and will graduate in 2023 as a lab technician. "],["loading-sorting-and-visualising-data.html", "2 loading, sorting and visualising data 2.1 analysing the data:", " 2 loading, sorting and visualising data For both readability and data analysis is it important to keep data tidy. Especially when loading data in, it is important to check if the format is tidy for the data analysis that will be conducted. In this chapter, I will show an example of how I loaded in an excel document, selected the needed data and made the used tables tidy. The excel document contains data from an experiment with C. Elegans. In this experiment, the influence of 3 substances on the growth of C. Elegans is tested. Further more, there is a positive and negative control. The positive control for this experiments is incubated in ethanol, the negative control for this experiment is incubated in s-medium. The raw data is shown in figure 2.1 Figure 2.1: table with raw data based upon the names and values, as shown in figure 2.1, the expected data-types are: - Raw-data: numeric - compNames: character - compConcentration: numeric In figure 2.1, Only the raw-data is numeric, both the compNames and compConcentrations are character strings. This would suggest that something went wrong during importation of the samples. This is also seen in figure 2.2, where the data was not sorted based upon the actual size, because it is being read as character and therefor is literaly sorted based upon the characters. Figure 2.2: graph without changing data type When correcting the data type of the compound concentration back to numeric, the graph in figure 2.3 can be created. Figure 2.3: graph with changing data type In this graph, the data is still rather spread out. This can be corrected for by using the negative control as a scale of 1. This brings the data in the plot relatively closer together, making it easier to compare data of different groups. The graph would than look like this: i really dunno how this graph would look, because the negative control has a value of 0, therefor scaling the exsisting data to this does not seem possible. 2.1 analysing the data: To determine if there is a statistical significant difference between the control groups and the experimental group, the following tests can be conducted on the data set: make a simple dotted plot (already done) and look at how the data is roughly distributed. check if the controls of the experiments have the expected result (positive control is positive, negative control is negative) check if the data of each group is normally distributed (shapiro-wilk) compare the experimental group to the (positive) control group with the propper statistical tests. these would include: Levenes test of equality of error variances 2 sides unpaired t.test However, as analysis the entire data set is not the goal of this chapter, the further data analysis will not be conducted. "],["reproducibility.html", "3 reproducibility 3.1 paper1 3.2 paper2", " 3 reproducibility Making sure that a research is reproducible, is an important measure to avoid fraud and generating results based on the general interest of a person or company. A big step towards making research reproducible is the movement towards open science, where the methods of both research and the data analysis are shared. In my opinion, open research helps to speed up progression and holds potential to decrease fraud. Therefor, I support the movement towards open science. In this chapter, I will review 2 papers, to show that I am able to determine how open the researchers are about their methods for both the laboratory work and the data analysis. The first paper will be a more traditional research paper, and is found on pubmed. This paper will be reviewed based upon following Repita criteria, as shown in figure 3.1. The second paper, will be a r code paper, and is found on the OSF website. The review of this paper will focus more heavily on the readability and reproducibility of the r code and will be less centered around the criteria shown in figure 3.1 Figure 3.1: table with repita criteria for reviewing articles 3.1 paper1 The following article will be reviewed: S. Ahmed, et all. (2-12-2020), A five-day course of ivermectin for the treatment of COVID-19 may reduce the duration of illness, read at 30-04-2022 from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7709596/ this study involved 72 patients from Dhaka, Bangladesh (criteria were age 1865 years;admitted to hospital within the last 7 days; presence of a fever (37.5 °C), cough, and/or sore throat; diagnosed positive for SARS-CoV-2) the patients were seperated into 3 groups: - oral ivermectin alone (12 mg once daily for 5 days) - oral ivermectin in combination with doxycycline (12 mg ivermectin single dose and 200 mg doxycycline on day 1, followed by 100 mg every 12 h for the next 4 days) - a placebo control group. The primary endpoints that were measured were the time required for virological clearance, and the remission of fever (37.5 °C) and cough within 7 days. in conclusion, the study found that Virological clearance was earlier in the 5-day ivermectin treatment arm when compared to the placebo group (9.7 days vs 12.7 days; p = 0.02). However, the virological clearance was not lower for the ivermectin + doxycycline arm (11.5 days; p = 0.27). There were no severe adverse drug events recorded in the study. A 5-day course of ivermectin was found to be safe and effective in treating adult patients with mild COVID-19. Larger trials, however, would still be needed to confirm these preliminary findings. 3.1.1 study purpose: the study purpose was clearly destribed in the introduction: The study was performed to evaluate the rapidity of viral clearance and safety of a 5-day course of ivermectin or a single-dose of ivermectin and a 5-day course of doxycycline in the treatment of mild COVID-19 in adults. 3.1.2 data availibality/location: there was no data availability statement found, neither was the (raw) data found anywhere linked by the article. 3.1.3 study location: the study location, and origin of the patients has been given. The criteria which had to be met for patients to take place in the research has also been listed. 3.1.4 author review: proper information of how to contact the authors has been provided. 3.1.5 ethics statement: there is an ethics statement in the study, for aproval of the experimental setup by both medical institutes and the patients. However, no further mention has been made of ethics regarding the experimental data. 3.1.6 funding statement: A funding statement was provided, the study was funded by Beximco Pharmaceutical Limited, Bangladesh. 3.1.7 code availability: no mention of code has been made. 3.1.8 conclusion: in conclusion: this paper has properly described their findings, sources and conclusions. However, the raw/processed data is not available nor is the code with which the data was analysed. Therefor, this paper is not entirely open science and can still improve, by providing the data and the used code to analyse the data. 3.2 paper2 the second article that will be reviewed is: https://osf.io/6pxjh/ In this paper, is tested if appointed local executives in Russian voting will produce more election manipulation for national parties and if this comes at a cost to the partys un-manipulated support. To test this theory, the study will use an election-forensic analysis of precinct-level election data from 176 Russian cities over six national elections from 2003 to 2012. the provided r code appears to have a few goals: - making the data tidy (removing na, proper row/colum orginisation for the data analysis, etc.) - visualizing and interpreting the data - creating marginal structural models to account for selection bias and confounding. The code has a good readability. It is properly spaced out, and lines of code are provided with comments to explain the code. However, not all code lines have comments. This makes some of the code, sightly harder to read. Therefor, in terms of readability, I rate the code of this paper a 4 out of 5. After downloading and running the code, I hardly had to make any changes at all to let the code run. The only changes that were required, were: - installing the packages I didnt have yet with the install.package() function. - specify where I stored the data files on my computer With the provided code, I was also easily able to recreate the images that were in the original paper. An example of this, is shown in figure 3.2. This was figure2 in the original paper. Because it was so easy to run the code, and recreate the same figures, the reproducibility of this code gets a 5 out of 5. The used code is further shown down bellow figure 3.2. Figure 3.2: recreated graph from reviewed paper 3.2.1 code paper 2 library(tidyverse) library(interactions) library(stargazer) library(fastDummies) full.data &lt;- read.csv(&quot;raw/coefs with city covariates cleaned.csv&quot;) precincts_total &lt;- read.csv(&quot;raw/n_precincts_all.csv&quot;) full.data &lt;- full.data %&gt;% mutate(presidential = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, 1, 0)) full.data &lt;- full.data %&gt;% mutate(known.opp.mayor = ifelse(is.na(opp_mayor1) == T, 0, ifelse(opp_mayor1==1, 1,0))) full.data &lt;- full.data %&gt;% mutate(appointed.exec = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x &gt;= cancel.year, 1, 0))) full.data &lt;- full.data %&gt;% dplyr::rename(turnout.coef = turnout) full.data &lt;- full.data %&gt;% mutate(years.with.elections = ifelse(is.na(cancel.year)==T, year.x-1996, ifelse(year.x - cancel.year &lt; 0, year.x - 1996, cancel.year - 1996))) #This gives a measure of early vs late vs. control; 1996 bc this is when local elections first took place full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x - cancel.year &lt; 0, 0, year.x - cancel.year))) full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2007 &amp; year.x ==2007, 1, years.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2011 &amp; year.x ==2011, 1, years.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x - cancel.year &lt; 0, 0, 1))) full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2007 &amp; year.x ==2007, 1, treated.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2011 &amp; year.x ==2011, 1, treated.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(years.under.appt = ifelse(is.na(cancel.year) == T, 0, ifelse(year.x - cancel.year &lt; 0, 0, ifelse(year.x == 2007 | year.x == 2011 &amp; year.x == cancel.year, 1, year.x - cancel.year)))) full.data &lt;- full.data %&gt;% mutate(city_id_factor = as.factor(city_id)) full.data &lt;- full.data %&gt;% mutate(treatment.group = ifelse(is.na(cancel.year)==F, 1, 0)) full.data &lt;- full.data %&gt;% mutate(control = ifelse(is.na(cancel.year)==T, 1, 0)) full.data &lt;- full.data %&gt;% mutate(early.treated = ifelse(is.na(cancel.year)==F &amp; cancel.year &lt;= 2007, 1, 0)) full.data &lt;- full.data %&gt;% mutate(late.treated = ifelse(is.na(cancel.year)==F &amp; cancel.year &gt; 2007, 1, 0)) full.data &lt;- full.data %&gt;% mutate(years.under.regime = ifelse(appointed.exec == 0, year.x - 2000, year.x - cancel.year)) ##Dummies for ethnic categorical full.data &lt;- dummy_cols(full.data, select_columns = &quot;ethnic.categorical&quot;) full.data &lt;- full.data %&gt;% dplyr::rename(majority.russian.republic = ethnic.categorical_Majority_Russian_republic) full.data &lt;- full.data %&gt;% dplyr::rename(majority.minority.republic = `ethnic.categorical_Majority-minority_republic`) ##Adding n_precincts precincts_total &lt;- precincts_total %&gt;% mutate(city_id_year = paste(city_id, year, sep = &quot;_&quot;)) precincts_total &lt;- precincts_total %&gt;% dplyr::select(-year, -city_id) full.data &lt;- left_join(full.data, precincts_total, by =&quot;city_id_year&quot;) full.data &lt;- full.data %&gt;% dplyr::rename(n.precincts = n) ##Share of appointed mayors nationwide full.data &lt;- full.data %&gt;% group_by(year.x) %&gt;% mutate(total.appointed = sum(appointed.exec, na.rm=T)) full.data &lt;- full.data %&gt;% group_by(year.x) %&gt;% mutate(total.cities = length(unique(city_id))) full.data &lt;- full.data %&gt;% mutate(appointed.share = total.appointed/total.cities) ## Creating &#39;.mod&#39; variables, which are lagged one year for presidential election years full.data &lt;- full.data %&gt;% mutate(urgov.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(urgov, 1), urgov)) full.data &lt;- full.data %&gt;% mutate(mayor.tenure.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(mayor.tenure, 1), mayor.tenure)) full.data &lt;- full.data %&gt;% mutate(lnAvgSalary.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(lnAvgSalary, 1), lnAvgSalary)) full.data &lt;- full.data %&gt;% mutate(dem.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(dem, 1), dem)) full.data &lt;- full.data %&gt;% mutate(comp.scale.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(comp.scale2, 1), comp.scale2)) 3.2.1.1 Overall manipulation model.tc2 &lt;- lm(turnout.coef ~ factor(city_id) + factor(year.x) + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + appointed.exec, data = full.data) #summary(model.tc2) #sjPlot::plot_model(model.tc2, type = &quot;int&quot;, show.data = T) #sim_slopes(model.tc2, pred = appointed.exec, modx = years.under.regime, jnplot = TRUE) model.tc.base &lt;- lm(turnout.coef ~ factor(city_id) + presidential + xconst.lag + putin.app.3mo + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + appointed.exec, data = full.data) #summary(model.tc.base) ##Plot base model p.tc.base &lt;- sjPlot::plot_model(model.tc.base, colors = &quot;bw&quot;, terms = c(&quot;presidential&quot;, &quot;xconst.lag&quot;, &quot;putin.app.3mo&quot;, &quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;)) + ylim(-.45, 1.65) + labs(title = &quot;Overall manipulation&quot;) + theme_bw() + geom_hline(aes(yintercept = 0)) p.tc.base &lt;- p.tc.base + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;, &quot;Putin approval&quot;, &quot;Exec. constraints&quot;, &quot;Presidential election&quot;)) ##Plot second model p.tc.2 &lt;- sjPlot::plot_model(model.tc2, colors = &quot;bw&quot;, terms = c(&quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;)) + ylim(-.45, .45) + labs(title = &quot;Overall manipulation&quot;) + theme_bw() + geom_hline(aes(yintercept = 0)) p.tc.2 &lt;- p.tc.2 + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;)) #Write the coefficient names in backwards order, because the plot is transposed 3.2.1.2 UR voteshare model.ur2 &lt;- lm(ur.voteshare ~ factor(city_id) + factor(year.x) + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + turnout.coef + appointed.exec, data = full.data) #summary(model.ur2) #sjPlot::plot_model(model.ur2, type = &quot;int&quot;) model.ur.base &lt;- lm(ur.voteshare ~ factor(city_id) + presidential + xconst.lag + putin.app.3mo + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + turnout.coef + appointed.exec, data = full.data) #summary(model.ur.base) p.ur.base &lt;- sjPlot::plot_model(model.ur.base, colors = &quot;bw&quot;, terms = c(&quot;presidential&quot;, &quot;xconst.lag&quot;, &quot;putin.app.3mo&quot;, &quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;years.under.regime&quot;, &quot;turnout.coef&quot;, &quot;appointed.exec&quot;)) + ylim(-.25, .5) + labs(title = &quot;United Russia vote-share&quot;) + geom_hline(yintercept=0) + theme_bw() p.ur.base &lt;- p.ur.base + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Turnout coef.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;, &quot;Putin approval&quot;, &quot;Exec. constraints&quot;, &quot;Presidential election&quot;)) p.ur.2 &lt;- sjPlot::plot_model(model.ur2, colors = &quot;bw&quot;, terms = c(&quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;, &quot;turnout.coef&quot;)) + ylim(-.1, .5) + labs(title = &quot;United Russia vote-share&quot;) + geom_hline(yintercept=0) + theme_bw() p.ur.2 &lt;- p.ur.2 + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Turnout coef.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;)) 3.2.1.3 Table stargazer(model.tc.base, model.tc2, model.ur.base, model.ur2, type = &quot;html&quot;, digits = 2, omit = &quot;factor&quot;, out = &quot;table_main.html&quot;) 3.2.1.4 Multiplot # Multiple plot function # # ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects) # - cols: Number of columns in layout # - layout: A matrix specifying the layout. If present, &#39;cols&#39; is ignored. # # If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), # then plot 1 will go in the upper left, 2 will go in the upper right, and # 3 will go all the way across the bottom. # multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } 3.2.1.5 Plotting png(filename = &quot;coefplot manipulation multiple.png&quot;, width = 7, height = 5, res = 500, units = &quot;in&quot;) multiplot(p.tc.base, p.tc.2, cols = 1) dev.off() png(filename = &quot;coefplot voteshare multiple.png&quot;, width = 7, height = 5, res = 500, units = &quot;in&quot;) multiplot(p.ur.base, p.ur.2, cols = 1) dev.off() "],["ordering-datafiles.html", "4 ordering datafiles 4.1 directory trees 4.2 version control", " 4 ordering datafiles Besides keeping a data file tidy, it is also important to keep the folders in which these data files are stored tidy. A properly made directory with version control, helps to avoid that data gets lost. It also generally improves speed and reproducibility. 4.1 directory trees In directory tree bellow, I showed the directory tree from this portfolio, as an example of how I think data can be stored tidy. ## ~/documents/hu/R/DSFB2/portofolio ## +-- 01_portofolio_opdracht1.1.rmd ## +-- 02_portofolio_opdracht1.2.rmd ## +-- 03_portofolio_opdracht2.Rmd ## +-- 04_portofolio_opdracht3.2.Rmd ## +-- 06_portfokio_opdracht5.Rmd ## +-- bibliography.bib ## +-- CV_files ## | +-- font-awesome-5.1.0 ## | | +-- css ## | | | +-- all.css ## | | | \\-- v4-shims.css ## | | \\-- webfonts ## | | +-- fa-brands-400.eot ## | | +-- fa-brands-400.svg ## | | +-- fa-brands-400.ttf ## | | +-- fa-brands-400.woff ## | | +-- fa-brands-400.woff2 ## | | +-- fa-regular-400.eot ## | | +-- fa-regular-400.svg ## | | +-- fa-regular-400.ttf ## | | +-- fa-regular-400.woff ## | | +-- fa-regular-400.woff2 ## | | +-- fa-solid-900.eot ## | | +-- fa-solid-900.svg ## | | +-- fa-solid-900.ttf ## | | +-- fa-solid-900.woff ## | | \\-- fa-solid-900.woff2 ## | \\-- paged-0.18 ## | +-- css ## | | \\-- resume.css ## | \\-- js ## | +-- config.js ## | +-- hooks.js ## | \\-- paged.js ## +-- html ## | +-- CV.html ## | \\-- portofolio_opdracht3.2.html ## +-- images ## | +-- coefplot_manipution_multiple.png ## | \\-- image.JPG ## +-- index.Rmd ## +-- portofolio.Rproj ## +-- r ## | \\-- set_up_script.R ## +-- raw ## | +-- CE.LIQ.FLOW.062_Tidydata.xlsx ## | +-- coefs with city covariates cleaned.csv ## | +-- n_precincts_all.csv ## | \\-- table_assignment1.2.xlsx ## +-- README.md ## +-- rmd ## | +-- CV.rmd ## | +-- ols models.nb.html ## | +-- ols models.Rmd ## | \\-- table_main.html ## +-- _book ## | +-- 404.html ## | +-- aside.html ## | +-- free-studying-period.html ## | +-- images ## | | +-- coefplot_manipution_multiple.png ## | | \\-- image.JPG ## | +-- index.html ## | +-- libs ## | | +-- anchor-sections-1.1.0 ## | | | +-- anchor-sections-hash.css ## | | | +-- anchor-sections.css ## | | | \\-- anchor-sections.js ## | | +-- crosstalk-1.2.0 ## | | | +-- css ## | | | | \\-- crosstalk.min.css ## | | | \\-- js ## | | | \\-- crosstalk.min.js ## | | +-- datatables-binding-0.23 ## | | | \\-- datatables.js ## | | +-- datatables-css-0.0.0 ## | | | \\-- datatables-crosstalk.css ## | | +-- dt-core-1.11.3 ## | | | +-- css ## | | | | +-- jquery.dataTables.extra.css ## | | | | \\-- jquery.dataTables.min.css ## | | | \\-- js ## | | | \\-- jquery.dataTables.min.js ## | | +-- gitbook-2.6.7 ## | | | +-- css ## | | | | +-- fontawesome ## | | | | | \\-- fontawesome-webfont.ttf ## | | | | +-- plugin-bookdown.css ## | | | | +-- plugin-clipboard.css ## | | | | +-- plugin-fontsettings.css ## | | | | +-- plugin-highlight.css ## | | | | +-- plugin-search.css ## | | | | +-- plugin-table.css ## | | | | \\-- style.css ## | | | \\-- js ## | | | +-- app.min.js ## | | | +-- clipboard.min.js ## | | | +-- jquery.highlight.js ## | | | +-- plugin-bookdown.js ## | | | +-- plugin-clipboard.js ## | | | +-- plugin-fontsettings.js ## | | | +-- plugin-search.js ## | | | \\-- plugin-sharing.js ## | | +-- htmlwidgets-1.5.4 ## | | | \\-- htmlwidgets.js ## | | \\-- jquery-3.6.0 ## | | \\-- jquery-3.6.0.min.js ## | +-- loading-sorting-and-visualising-data.html ## | +-- main.html ## | +-- ordering-datafiles.html ## | +-- project-liquid-biopsies.html ## | +-- reproducibility.html ## | +-- search_index.json ## | \\-- _main_files ## | \\-- figure-html ## | +-- graph1-1.png ## | +-- graph2-1.png ## | +-- showing correct grap c elegans-1.png ## | \\-- showing wrong graph c elegans-1.png ## +-- _bookdown_files ## +-- _main.Rmd ## \\-- _main_files ## \\-- figure-html ## +-- graph1-1.png ## +-- graph2-1.png ## +-- showing correct grap c elegans-1.png ## \\-- showing wrong graph c elegans-1.png Another example of how a directory tree can look is shown in figure bellow. This is one of my directories of a previous school project. ## ~/documents/hu/R/DSFB2/daur2 ## +-- eindopdracht ## | +-- html ## | | \\-- eindopdracht.html ## | +-- images ## | | +-- image1_summaries_fastqc_analysis.png ## | | +-- image2_per_base_sequence_quality_fastqc_analysis.png ## | | \\-- image3_per_sequence_quality_fastqc_analysis.png ## | +-- raw ## | | \\-- identifyer.txt ## | +-- readme.txt ## | +-- rmd ## | | \\-- eindopdracht.Rmd ## | \\-- scripten ## | \\-- set_up.R ## +-- meta_genomics ## | +-- html ## | | +-- formatieve_opdracht.html ## | | \\-- les_opdrachten.html ## | +-- raw ## | +-- readme.txt ## | +-- rmd ## | | +-- formatieve_opdracht.Rmd ## | | \\-- les_opdrachten.Rmd ## | \\-- scripten ## | \\-- set_up.R ## \\-- rnaseq ## +-- html ## | +-- formatieve_opdracht.html ## | \\-- les_opdrachten.html ## +-- raw ## +-- readme.txt ## +-- rmd ## | +-- formatieve_opdracht.Rmd ## | \\-- les_opdrachten.Rmd ## \\-- scripten ## \\-- set_up.R 4.2 version control besides ordering the data structures on my own computer, I use version control with github for both my own repositories as well as those which I work on in projects. Working with github on group projects has helped me in many ways. It makes it far easier to work on the same documents together. Further more, we can far easier merge files, without the need for many different in between verions of the same file. This imrpoves both the speed, accuracy and communication within our projects. "],["free-studying-period.html", "5 free studying period 5.1 planning and organisation to learn python", " 5 free studying period I have taken a course data science during the minor of my study. One of our assingments was to set our own learning goals for skills we want to obtain in the future. During my minor, I spend about 30-40 hours on getting started with learning these new skills. The aim was to get started, and to lay a fundation on which I could build further after I graduated. On this page, I will describe what I choose to do with this free learning period and how I spend my time on. I got started by asking myself what I wanted to do and Where I wanted to be in ~2 years time? It was during this time that I realized I still have multiple things I wanna learn and develop myself in. These include: working in an ecological lab clinical lab work (like labratory work in a hospital) working in a project with organoids Because there are still many things that interest me, and I still have multiple jobs I would like to try, it was difficult for me to set my sites on a specific goal. Especially since Im not certain if I still want to continue working in the field of bioinformatics in the future. Therefor, I decided to take a more general approach and get a wider few of whats all still possible for me with bioinformatics. This would help me to more easily adept, and learn specific skills later when Ive a more clear vision of what I want to do. To generally widen my view, I decided to learn python. This would be useful to more easily flow into jobs where python is a required programming language. Further more, python is a very useful language for software creation, which I would like to learn, to create my own software at some point in the future. This latter use is not necessarily needed for my job within the life sciences, but, certainly could be an useful skill to obtain nevertheless. 5.1 planning and organisation to learn python I didnt really make any further plans yet. In general I wanted to follow a crashcourse, or read a book, and show case here what Ive learned and how I can now use python to get started on software creation and for general manipulation of data (files). Would love to hear your thoughts on this, for whoever is reviewing my portfolio right now. Thanks in advance, appreciate any feedback I can use to improve :) "],["project-liquid-biopsies.html", "6 project liquid biopsies 6.1 background 6.2 project 6.3 refrences", " 6 project liquid biopsies Between May and June, 2022, I wrote R-code for the Princess Máxima Center within a project called liquid biopsies. I worked on this project together with 2 other students: Pedro de Bos and Thijmen van Brenk. On this page, I will provide a short introduction with regards to why the research was important. I will also give a short summary of the r-code we eventually wrote. 6.1 background Neuroblastoma is an embryonic malignancy that affects normal development of the adrenal medulla and para vertebral sympathetic ganglia in early childhood. Despite extensive studies to the molecular characteristics of human neuroblastomas, the initiation mechanisms and even its origin are still largly unknown. Maris (2010) However, neuroblastoma is the most common extracranial solid tumor in children. Further more, it has high recurrence rates and low survival rates. Therefore, early diagnosis, treatment response evaluation, and recurrence monitoring are of great significance for NB patients.  (2022) A further complication is that neuroblastoma exhibits genetic, morphological and clinical heterogeneity. This in term limits the efficacy of existing treatment. Therefor, Gaining detailed knowledge of the molecular signatures and genetic variations involved in the pathogenesis of neuroblastoma is necessary to develop safer and more effective treatments for neuroblastomas. Zafar et al. (2021) The wide genetic variation, also means that several different kinds of cell progresses can be effected. These include changes in the regulatory role in differentiation, apoptosis, cell proliferation, tumourigenesis, angiogenesis or metastasis of neuroblastoma Aygun (2018) One of the isseus with neuroblastoma is that to properly monitor the progression of the tumor, samples need to be taken often. And tumor biopsies can be very invasive. A possible solution, lays with liquid biopsies. Liquid biopsies, is a method of obtaining genomic material of tumors from blood samples. tumors secret DNA to the blood, which is called cell free DNA. This cell free DNA can be amplified, sequences and analysed. A study has shown that the proportion of tumor-derived DNA in cell free DNA was 42.5% (16.9%-55.9% across all samples). This could potentially mean sufficient sensitivity of liquid biopsy for neuroblastoma.@shiraiQuantitativeAssessmentCopy2022 6.2 project At this moment, data sets of DNA from tumor biopsies and liquid biopsies of patients with neuoblastoma are compared manual at the Princes Maxima Center. This is very time consuming, and when working with time pressure from an increasing number of patients, can lead to human error. Therefor, using shinny, we created a web database which allows for easy comparing and visualization of sequencing results. 6.3 refrences "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
