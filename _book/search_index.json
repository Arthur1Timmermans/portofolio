[["index.html", "portfolio Arthur Timmermans 1 prerequisites", " portfolio Arthur Timmermans Arthur Timmermans 1 prerequisites Welcome to my personal portfolio. On this website I will give more inside into what my skills are and which projects I work(ed) on. I am a life science student at the Hoge School Utrecht (HU), and will graduate in 2023 as a lab technician. Within the science field, not just for data analysiss, but also for science in a whole, I strongly believe in the movement towards open science. In my personal opinion, being open about our research and sharing our knowledge and experience will help to reunite people. And when we work together, feats that seem impossible right now suddenly become achievable. While making this website, I worked in a github repository in RMD files. To see the underlying code of my website, you can visit my github reposetory. Make sure to read the readme on the home page for further instructions on how to use my github repository. For those who are interested in getting more underlying information about me, I creating a resume with R and this is shown bellow. This resume was build using a template from the pagedown package. The RMD file that I used to create my resume can be found here. This file was also knitted to a html file and an image resume "],["loading-sorting-and-visualising-data.html", "2 loading, sorting and visualising data 2.1 analysing the data:", " 2 loading, sorting and visualising data For both readability and data analysis is it important to keep data tidy. Especially when loading data in, it is important to check if the format is tidy for the data analysis that will be conducted. In this chapter, I will show an example of how I loaded in an excel document, selected the needed data and made the used tables tidy. The excel document contains data from an experiment conducted by by J. Louter (INT/ILC).In this experiment, the influence of 3 substances on the growth of C. Elegans is tested for their toxicity.The more growth is inhibited, the more toxic the substance is expected to be. The substances that were tested are: 2,6 disopropylnaphealene decane naphthalene Further more, there is a positive and negative control. The positive control for this experiments is incubated in ethanol, which is expected to be very toxic and inhibit the growth of C. elegans a lot. The negative control for this experiment is incubated in s-medium, which is expected to offer proper living conditions and allow C. elegans to grow and live. The raw data is shown in figure 2.1. Within this table: The expType column shows if the data is a experimental condition or a control The rawdata column shows the living C. elegans. The compName Column shows the compound name The compConcentration column shows the compound concentration. More metadata is available in the original Excel sheet, which can be found here. However, in figure 2.1 only the main variables that will be worked with are shown for readability. Figure 2.1: table with raw data from c. elegans project based upon the names and values, as shown in figure 2.1, the expected data-types are: expType: character Raw-data: numeric compNames: character compConcentration: numeric In figure 2.1, Only the raw-data is numeric, both the compNames, compConcentrations and expType are character strings. This would suggest that something went wrong during importation of the samples. This is also seen in figure 2.2, where the data was not sorted based upon the actual size, because it is being read as character and therefor is literally sorted based upon the characters.This is an error that occurs more often when importing files into a R object, and showcases why it is important to check the datatypes after importing data. Figure 2.2: graph without changing data type When correcting the data type of the compound concentration back to numeric, the graph in figure 2.3 can be created. In this graph, the X-axis shows the compound concentration in a log value. A log value was chosen because on a normal scale, the spread was very difficult to see. The Y axis shows the amount of living C. elegans. On the left side of figure 2.3, the negative control in s medium grew properly. This showcases that the C. elegans nematodes grew as expected. The positive control with ethanol also seems to show a negative correlation between the concentration of ethanol and the living C. elegans. This would showcase that the principal to test the toxicity by correlating concentration of a compound to the amount of living C. elegans worked in this assay. The other compounds also seem inhibit the growth of C. elegans as the compound concentration increases. Figure 2.3: graph with changing data type In this 2.3, the data on the Y axis is still rather spread out. This can be corrected for by using the negative control as a scale of 1 on the Y axis. This brings the data in the plot relatively closer together, making it easier to compare data of different groups too the negative control. The graph would than look like shown in figure 2.4. In this plot the negative correlation between living C. elegans and the increase in compound concentration again seems to become visible. However, because no statistical tests are run yet, the correlation can not officially be claimed to be present yet. Figure 2.4: graph with normalized scale 2.1 analysing the data: to further determine the (toxic) effect of the added substances, a dose-response analysis can be run. In this analysis, a log-logistic model can be used, such as the glm function of R. In this model, each compound would be tested separately and the the amount of living C. elegans and the compound concentration would be used as input. the model would return data including: estimated maximal estimated minimal The output data from the model can be used in the dose.p function from the MASS library. This function will calculate any fractional dosage value, including the the IC50 concentration Now the slope at IC50 can also be calculated, and put into a dot-plot with a trend line. The data can also be put in a box plot, which can also help to check for outliers. If outliers would be suspected, this could be tested with the rosnerTest() function from the {EnvStats} package. This specific test is excellent for large data sets (population larger than 40). If a relatively small part of the data set is tested for outliers (population smaller than 40), the dixon.test() function from the {outliers} package could be used. The different graphs from different compounds can be compared to the positive and negative control and to each other, to estimate if there is a difference in toxicity. If a difference is suspected, the calculated values, including the maximal, minimal and IC 50 can be compared. This comparison can be done by: testing for normality: levenes test for normality if normaly distrubuted: unpaired t.test if not normally distributed: Mann-Whitney U-test compare found P-values. if P is smaller than 0,05 accept alternative hypothesis, else accept null hypothesis However, as analysis the entire data set is not the goal of this chapter, the further data analysis will not be conducted. "],["reproducibility.html", "3 reproducibility 3.1 paper1 3.2 paper2", " 3 reproducibility Making sure that research is reproducible, is an important measure to avoid fraud and generating results based on the general interest of a person or company. A big step towards making research reproducible is the movement towards open science, where the methods of both research and the data analysis are shared. In my opinion, open research helps to speed up progression and holds potential to decrease fraud. Therefor, I support the movement towards open science. In this chapter, I will review 2 papers, to show that I am able to determine how open the researchers are about their methods for both the laboratory work and the data analysis. The first paper will be a more traditional research paper, and is found on pubmed. This paper will be reviewed based upon following Repita criteria, as shown in figure 3.1. The second paper, will be a r code paper, and is found on the OSF website. The review of this paper will focus more heavily on the readability and reproducibility of the r code and will be less centered around the criteria shown in figure 3.1 Figure 3.1: table with repita criteria for reviewing articles 3.1 paper1 The following article will be reviewed: (ahmedFivedayCourseIvermectin2021?) this study involved 72 patients from Dhaka, Bangladesh (criteria were age 1865 years;admitted to hospital within the last 7 days; presence of a fever (37.5 °C), cough, and/or sore throat; diagnosed positive for SARS-CoV-2) the patients were seperated into 3 groups: - oral ivermectin alone (12 mg once daily for 5 days) - oral ivermectin in combination with doxycycline (12 mg ivermectin single dose and 200 mg doxycycline on day 1, followed by 100 mg every 12 h for the next 4 days) - a placebo control group. The primary endpoints that were measured were the time required for virological clearance, and the remission of fever (37.5 °C) and cough within 7 days. in conclusion, the study found that Virological clearance was earlier in the 5-day ivermectin treatment arm when compared to the placebo group (9.7 days vs 12.7 days; p = 0.02). However, the virological clearance was not lower for the ivermectin + doxycycline arm (11.5 days; p = 0.27). There were no severe adverse drug events recorded in the study. A 5-day course of ivermectin was found to be safe and effective in treating adult patients with mild COVID-19. Larger trials, however, would still be needed to confirm these preliminary findings. 3.1.1 study purpose: The study purpose was clearly described in the introduction. Locating the purpose was easy as well. The authors/article did a good job at this. The purpose was: The study was performed to evaluate the rapidity of viral clearance and safety of a 5-day course of ivermectin or a single-dose of ivermectin and a 5-day course of doxycycline in the treatment of mild COVID-19 in adults. 3.1.2 data and code availibality/location: there was no data availability statement found, neither was the (raw) data found anywhere linked in the article. The code was also never mentioned in the article. This is not uncommon for these kinds of articles, however, it makes the article less reproducible and puts it out of contention for the movement of open science. 3.1.3 study location: the study location, and origin of the patients has been given. The criteria which had to be met for patients to take place in the research has also been listed. I provided these in the introduction above. Even though the data itself was not provided, tracing the locations of the study is now still possible which is a proper move towards open science. 3.1.4 author review: proper information of how to contact the authors has been provided in the paper, mail adresses of all authors were available. The professionalism of the authors could also be verified. In my opinion, the authors seemed professional and this makes the findings of the article more reliable. 3.1.5 ethics statement: there is an ethics statement in the study, for approval of the experimental setup by both medical institutes and the patients. However, no further mention has been made of ethics regarding the experimental data. 3.1.6 funding statement: A funding statement was provided, the study was funded by Beximco Pharmaceutical Limited, Bangladesh. 3.1.7 conclusion: in conclusion: this paper has properly described their findings, sources and conclusions.Somethings like the professionalism of the authors and the funding/interest of the project can be verified. However, the raw/processed data is not available nor is the code with which the data was analysed. Therefor, this paper is not entirely open science and can still improve, by providing the data and the used code to analyse the data. 3.2 paper2 the second article that will be reviewed is: (harveyWhoDeliversVotes2022?) multiparty elections provide a variety of benefits for regime survival. Specifically, the introduction of local elections in otherwise closed regimes has been shown to generate improvements in government legitimacy and performance. However, some non-democratic governments do not hold local executive elections, with the hopes of increasing control over local political resources. In this paper, is tested if appointed local executives in Russian voting will produce more election manipulation for national parties and if this comes at a cost to the partys un-manipulated support. To test this theory, the study will use an election-forensic analysis of precinct-level election data from 176 Russian cities over six national elections from 2003 to 2012. this paper proposes that appointed local executives will produce more election manipulation for national parties, but that this comes at a cost to the partys un-manipulated support. 3.2.1 analysing the code the provided r code appears to have a few goals: loading in data making the data tidy (removing NA, proper row/column organisation for the data analysis, etc.) visualizing and interpreting the data creating marginal structural models to account for selection bias and confounding. run the model and put the results into plots The code has a good readability. It is properly spaced out, and lines of code are provided with comments to explain the code. However, not all code lines have comments. This makes some of the code, sightly harder to read. Therefor, in terms of readability, I rate the code of this paper a 4 out of 5. After downloading and running the code, I hardly had to make any changes at all to let the code run. The only changes that were required, were: installing the packages I didnt have yet with the install.package() function. specify where I stored the data files on my computer With the provided code, I was also easily able to recreate the images that were in the original paper. An example of this, is shown in figure 3.2. This was figure2 in the original paper. In this figure, it is visible that Putin has a far higher manipulation than the other politicians and that appointed exec party also has sightly higher manipulation. Because it was so easy to run the code, and recreate the same figures, I would rate the reproducibility of this code a 5 out of 5. The used code is further shown down bellow figure 3.2. Figure 3.2: recreated graph from reviewed paper 3.2.2 raw code of paper 2 library(tidyverse) library(interactions) library(stargazer) library(fastDummies) ##################################################################################################################### # this is the only section that requires changes, add your own location where you saved the .csv files. full.data &lt;- read.csv(&quot;raw/coefs with city covariates cleaned.csv&quot;) precincts_total &lt;- read.csv(&quot;raw/n_precincts_all.csv&quot;) ##################################################################################################################### full.data &lt;- full.data %&gt;% mutate(presidential = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, 1, 0)) full.data &lt;- full.data %&gt;% mutate(known.opp.mayor = ifelse(is.na(opp_mayor1) == T, 0, ifelse(opp_mayor1==1, 1,0))) full.data &lt;- full.data %&gt;% mutate(appointed.exec = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x &gt;= cancel.year, 1, 0))) full.data &lt;- full.data %&gt;% dplyr::rename(turnout.coef = turnout) full.data &lt;- full.data %&gt;% mutate(years.with.elections = ifelse(is.na(cancel.year)==T, year.x-1996, ifelse(year.x - cancel.year &lt; 0, year.x - 1996, cancel.year - 1996))) #This gives a measure of early vs late vs. control; 1996 bc this is when local elections first took place full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x - cancel.year &lt; 0, 0, year.x - cancel.year))) full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2007 &amp; year.x ==2007, 1, years.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(years.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2011 &amp; year.x ==2011, 1, years.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==T, 0, ifelse(year.x - cancel.year &lt; 0, 0, 1))) full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2007 &amp; year.x ==2007, 1, treated.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(treated.post.treatment = ifelse(is.na(cancel.year)==F &amp; cancel.year == 2011 &amp; year.x ==2011, 1, treated.post.treatment)) #This codes cancelations in 2007 and 2011 as 1 for those years, since elections take place at year-end. full.data &lt;- full.data %&gt;% mutate(years.under.appt = ifelse(is.na(cancel.year) == T, 0, ifelse(year.x - cancel.year &lt; 0, 0, ifelse(year.x == 2007 | year.x == 2011 &amp; year.x == cancel.year, 1, year.x - cancel.year)))) full.data &lt;- full.data %&gt;% mutate(city_id_factor = as.factor(city_id)) full.data &lt;- full.data %&gt;% mutate(treatment.group = ifelse(is.na(cancel.year)==F, 1, 0)) full.data &lt;- full.data %&gt;% mutate(control = ifelse(is.na(cancel.year)==T, 1, 0)) full.data &lt;- full.data %&gt;% mutate(early.treated = ifelse(is.na(cancel.year)==F &amp; cancel.year &lt;= 2007, 1, 0)) full.data &lt;- full.data %&gt;% mutate(late.treated = ifelse(is.na(cancel.year)==F &amp; cancel.year &gt; 2007, 1, 0)) full.data &lt;- full.data %&gt;% mutate(years.under.regime = ifelse(appointed.exec == 0, year.x - 2000, year.x - cancel.year)) ##Dummies for ethnic categorical full.data &lt;- dummy_cols(full.data, select_columns = &quot;ethnic.categorical&quot;) full.data &lt;- full.data %&gt;% dplyr::rename(majority.russian.republic = ethnic.categorical_Majority_Russian_republic) full.data &lt;- full.data %&gt;% dplyr::rename(majority.minority.republic = `ethnic.categorical_Majority-minority_republic`) ##Adding n_precincts precincts_total &lt;- precincts_total %&gt;% mutate(city_id_year = paste(city_id, year, sep = &quot;_&quot;)) precincts_total &lt;- precincts_total %&gt;% dplyr::select(-year, -city_id) full.data &lt;- left_join(full.data, precincts_total, by =&quot;city_id_year&quot;) full.data &lt;- full.data %&gt;% dplyr::rename(n.precincts = n) ##Share of appointed mayors nationwide full.data &lt;- full.data %&gt;% group_by(year.x) %&gt;% mutate(total.appointed = sum(appointed.exec, na.rm=T)) full.data &lt;- full.data %&gt;% group_by(year.x) %&gt;% mutate(total.cities = length(unique(city_id))) full.data &lt;- full.data %&gt;% mutate(appointed.share = total.appointed/total.cities) ## Creating &#39;.mod&#39; variables, which are lagged one year for presidential election years full.data &lt;- full.data %&gt;% mutate(urgov.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(urgov, 1), urgov)) full.data &lt;- full.data %&gt;% mutate(mayor.tenure.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(mayor.tenure, 1), mayor.tenure)) full.data &lt;- full.data %&gt;% mutate(lnAvgSalary.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(lnAvgSalary, 1), lnAvgSalary)) full.data &lt;- full.data %&gt;% mutate(dem.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(dem, 1), dem)) full.data &lt;- full.data %&gt;% mutate(comp.scale.mod = ifelse(year.x == 2004 | year.x == 2008 | year.x == 2012, lag(comp.scale2, 1), comp.scale2)) 3.2.2.1 Overall manipulation model.tc2 &lt;- lm(turnout.coef ~ factor(city_id) + factor(year.x) + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + appointed.exec, data = full.data) #summary(model.tc2) #sjPlot::plot_model(model.tc2, type = &quot;int&quot;, show.data = T) #sim_slopes(model.tc2, pred = appointed.exec, modx = years.under.regime, jnplot = TRUE) model.tc.base &lt;- lm(turnout.coef ~ factor(city_id) + presidential + xconst.lag + putin.app.3mo + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + appointed.exec, data = full.data) #summary(model.tc.base) ##Plot base model p.tc.base &lt;- sjPlot::plot_model(model.tc.base, colors = &quot;bw&quot;, terms = c(&quot;presidential&quot;, &quot;xconst.lag&quot;, &quot;putin.app.3mo&quot;, &quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;)) + ylim(-.45, 1.65) + labs(title = &quot;Overall manipulation&quot;) + theme_bw() + geom_hline(aes(yintercept = 0)) p.tc.base &lt;- p.tc.base + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;, &quot;Putin approval&quot;, &quot;Exec. constraints&quot;, &quot;Presidential election&quot;)) ##Plot second model p.tc.2 &lt;- sjPlot::plot_model(model.tc2, colors = &quot;bw&quot;, terms = c(&quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;)) + ylim(-.45, .45) + labs(title = &quot;Overall manipulation&quot;) + theme_bw() + geom_hline(aes(yintercept = 0)) p.tc.2 &lt;- p.tc.2 + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;)) #Write the coefficient names in backwards order, because the plot is transposed 3.2.2.2 UR voteshare model.ur2 &lt;- lm(ur.voteshare ~ factor(city_id) + factor(year.x) + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + turnout.coef + appointed.exec, data = full.data) #summary(model.ur2) #sjPlot::plot_model(model.ur2, type = &quot;int&quot;) model.ur.base &lt;- lm(ur.voteshare ~ factor(city_id) + presidential + xconst.lag + putin.app.3mo + urgov.mod + margin.most.recent + n.precincts + mayor.tenure.mod + lnAvgSalary.mod + dem.mod + comp.scale.mod + known.opp.mayor + years.under.regime + turnout.coef + appointed.exec, data = full.data) #summary(model.ur.base) p.ur.base &lt;- sjPlot::plot_model(model.ur.base, colors = &quot;bw&quot;, terms = c(&quot;presidential&quot;, &quot;xconst.lag&quot;, &quot;putin.app.3mo&quot;, &quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;years.under.regime&quot;, &quot;turnout.coef&quot;, &quot;appointed.exec&quot;)) + ylim(-.25, .5) + labs(title = &quot;United Russia vote-share&quot;) + geom_hline(yintercept=0) + theme_bw() p.ur.base &lt;- p.ur.base + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Turnout coef.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;, &quot;Putin approval&quot;, &quot;Exec. constraints&quot;, &quot;Presidential election&quot;)) p.ur.2 &lt;- sjPlot::plot_model(model.ur2, colors = &quot;bw&quot;, terms = c(&quot;urgov.mod&quot;, &quot;margin.most.recent&quot;, &quot;n.precincts&quot;, &quot;mayor.tenure.mod&quot;, &quot;lnAvgSalary.mod&quot;, &quot;dem.mod&quot;, &quot;comp.scale.mod&quot;, &quot;known.opp.mayor&quot;, &quot;appointed.exec&quot;, &quot;years.under.regime&quot;, &quot;turnout.coef&quot;)) + ylim(-.1, .5) + labs(title = &quot;United Russia vote-share&quot;) + geom_hline(yintercept=0) + theme_bw() p.ur.2 &lt;- p.ur.2 + scale_x_discrete(labels = c(&quot;Appointed exec.&quot;, &quot;Turnout coef.&quot;, &quot;Years under regime&quot;, &quot;Known opposition mayor&quot;, &quot;Regional competitivness&quot;, &quot;Regional political openness&quot;, &quot;Average salary&quot;, &quot;Mayor tenure&quot;, &quot;Number of precincts&quot;, &quot;Mayor&#39;s margin of victory&quot;, &quot;UR governor&quot;)) 3.2.2.3 Table stargazer(model.tc.base, model.tc2, model.ur.base, model.ur2, type = &quot;html&quot;, digits = 2, omit = &quot;factor&quot;, out = &quot;table_main.html&quot;) 3.2.2.4 Multiplot # Multiple plot function # # ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects) # - cols: Number of columns in layout # - layout: A matrix specifying the layout. If present, &#39;cols&#39; is ignored. # # If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), # then plot 1 will go in the upper left, 2 will go in the upper right, and # 3 will go all the way across the bottom. # multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } 3.2.2.5 Plotting png(filename = &quot;coefplot manipulation multiple.png&quot;, width = 7, height = 5, res = 500, units = &quot;in&quot;) multiplot(p.tc.base, p.tc.2, cols = 1) dev.off() png(filename = &quot;coefplot voteshare multiple.png&quot;, width = 7, height = 5, res = 500, units = &quot;in&quot;) multiplot(p.ur.base, p.ur.2, cols = 1) dev.off() "],["ordering-datafiles.html", "4 ordering datafiles 4.1 basic frame work 4.2 version control", " 4 ordering datafiles Besides keeping a data file tidy, it is also important to keep the folders in which these data files are stored tidy. A properly made directory with version control, helps to avoid that data gets lost. It also generally improves speed and reproducibility, especially when working in a team. 4.1 basic frame work To store, edit, analyse and maintain my data, I follow the basic Guerrilla analytics framework, written by Enda Ridge. The basic frame work is: Space is cheap, confusion is expensive Use simple, visual project structures and conventions Automate with program code Link stored data to data in the analytics environment to data in work products (rmarkdown) Version control changes to data and analytics code (Git/Github.com) Consolidate team knowledge (agree on guidelines and stick to it as a team) Use code that runs from start to finish The full description of Guarilla analytics framework can be found here, and it is recommendable to read through this for anyone who is not yet familiar with the framework. There are of course still plenty of other proper frame works designed for working tidy and reproducible. The key is in remaining consistent and sticking with something that works well for you. Therefor I also encourage doing your own research and finding a method that works well for you and your team. 4.1.1 directory tree of this project In the directory tree bellow, I showed the directory tree from this portfolio, as an example of how I think data can be stored tidy. A few notes of thing I try to keep track of: I avoid deep bedding (max 2-3 folder layers) to keep the relative path short Every folder is given a readme.txt file, giving a description of all files/folders that are present, the origin of the files, and the date at which the files are downloaded The name structure of the files is consistent and does not contain special caracters (accept for _ and . ) ## ~/documents/hu/R/DSFB2/portofolio ## +-- 01_portofolio_opdracht1.1.rmd ## +-- 02_portofolio_opdracht1.2.rmd ## +-- 03_portofolio_opdracht2.Rmd ## +-- 04_portofolio_opdracht3.2.Rmd ## +-- 06_portfokio_opdracht5.html ## +-- 06_portfokio_opdracht5.Rmd ## +-- 07_portfolio_opdracht7.Rmd ## +-- 08_portfolio_opdracht8.Rmd ## +-- 09_portfolio_opdracht9.Rmd ## +-- 10_refrences.Rmd ## +-- bibliography.bib ## +-- CV_files ## | +-- font-awesome-5.1.0 ## | | +-- css ## | | | +-- all.css ## | | | \\-- v4-shims.css ## | | \\-- webfonts ## | | +-- fa-brands-400.eot ## | | +-- fa-brands-400.svg ## | | +-- fa-brands-400.ttf ## | | +-- fa-brands-400.woff ## | | +-- fa-brands-400.woff2 ## | | +-- fa-regular-400.eot ## | | +-- fa-regular-400.svg ## | | +-- fa-regular-400.ttf ## | | +-- fa-regular-400.woff ## | | +-- fa-regular-400.woff2 ## | | +-- fa-solid-900.eot ## | | +-- fa-solid-900.svg ## | | +-- fa-solid-900.ttf ## | | +-- fa-solid-900.woff ## | | \\-- fa-solid-900.woff2 ## | \\-- paged-0.18 ## | +-- css ## | | \\-- resume.css ## | \\-- js ## | +-- config.js ## | +-- hooks.js ## | \\-- paged.js ## +-- data ## | +-- dengue_gapminder.csv ## | +-- dengue_tidy.csv ## | +-- dengue_tidy.rds ## | +-- flu_dengue.csv ## | +-- flu_gapminder.csv ## | +-- flu_tidy.csv ## | +-- flu_tidy.rds ## | +-- gapminder.csv ## | +-- gapminder.rds ## | \\-- readme ## +-- html ## | +-- cv.html ## | +-- ols models.nb.html ## | +-- readme.txt ## | \\-- table_main.html ## +-- images ## | +-- 09_plot1.png ## | +-- 09_plot2.png ## | +-- 09_plot3.png ## | +-- coefplot_manipution_multiple.png ## | +-- course1.png ## | +-- course2.png ## | +-- cv.png ## | +-- DB1.png ## | +-- DBD1.png ## | +-- DBF1.png ## | +-- DBG1.png ## | \\-- readme.txt ## +-- index.Rmd ## +-- portofolio.Rproj ## +-- python scripts ## | +-- calculator.py ## | +-- jupyter_notebook ## | | +-- annlyzing_watch_history.ipynb ## | | +-- drug200.csv ## | | +-- machine_learning_with_python.ipynb ## | | +-- metadata.xlsx ## | | +-- prison_break_dataset.ipynb ## | | \\-- Untitled.ipynb ## | +-- password-generator.py ## | +-- readme ## | +-- reminder-to-get-up.py ## | \\-- rock-paper-scisorcs.py ## +-- r ## | +-- installing_packages.R ## | +-- readme.txt ## | \\-- set_up_script.R ## +-- raw ## | +-- 007_dengue.csv ## | +-- 007_flu.csv ## | +-- CE.LIQ.FLOW.062_Tidydata.xlsx ## | +-- coefs with city covariates cleaned.csv ## | +-- data_covid19_cases.csv ## | +-- n_precincts_all.csv ## | +-- readme.txt ## | \\-- table_assignment1.2.xlsx ## +-- README.md ## +-- rmd ## | +-- cv.Rmd ## | +-- ols models.Rmd ## | \\-- readMe ## +-- SQL scripts ## | +-- joining_flu_dengue.sql ## | +-- joining_flu_gapminder.sql ## | +-- join_Dengue_gapminder.sql ## | +-- loading_data.sql ## | \\-- readMe ## +-- _book ## +-- _bookdown_files ## +-- _main.Rmd ## \\-- _main_files ## \\-- figure-html ## +-- 01graph1-1.png ## +-- 01graph2-1.png ## +-- 01graph3-1.png ## +-- 07graph1-1.png ## +-- 07graph2-1.png ## +-- 07graph3-1.png ## +-- 07graph4-1.png ## +-- 07graph5-1.png ## +-- 07graph6-1.png ## +-- 09graph1-1.png ## +-- 09graph2-1.png ## +-- 09graph3-1.png ## +-- 09graph4-1.png ## +-- example1-1.png ## +-- flu_graph1-1.png ## +-- graph1-1.png ## +-- graph2-1.png ## +-- graph3-1.png ## +-- showing correct grap c elegans-1.png ## +-- showing wrong graph c elegans-1.png ## +-- standard dengue graph-1.png ## +-- standard flu graph-1.png ## +-- together-1.png ## \\-- unnamed-chunk-2-1.png 4.1.2 directory tree of another project Another example of how a directory tree can look is shown in figure bellow. This is one of my directories of a previous school project. I recently made the format of this project tidy according to the principals mentioned above. ## ~/documents/hu/R/DSFB2/daur2 ## +-- eindopdracht ## | +-- html ## | | \\-- eindopdracht.html ## | +-- images ## | | +-- image1_summaries_fastqc_analysis.png ## | | +-- image2_per_base_sequence_quality_fastqc_analysis.png ## | | \\-- image3_per_sequence_quality_fastqc_analysis.png ## | +-- raw ## | | \\-- identifyer.txt ## | +-- readme.txt ## | +-- rmd ## | | \\-- eindopdracht.Rmd ## | \\-- scripten ## | \\-- set_up.R ## +-- meta_genomics ## | +-- html ## | | +-- formatieve_opdracht.html ## | | \\-- les_opdrachten.html ## | +-- raw ## | +-- readme.txt ## | +-- rmd ## | | +-- formatieve_opdracht.Rmd ## | | \\-- les_opdrachten.Rmd ## | \\-- scripten ## | \\-- set_up.R ## \\-- rnaseq ## +-- html ## | +-- formatieve_opdracht.html ## | \\-- les_opdrachten.html ## +-- raw ## +-- readme.txt ## +-- rmd ## | +-- formatieve_opdracht.Rmd ## | \\-- les_opdrachten.Rmd ## \\-- scripten ## \\-- set_up.R 4.2 version control besides ordering the data structures on my own computer, I use version control with github for both my own repositories as well as those which I work on in projects. Working with github on group projects has helped me in many ways. It makes it far easier to work on the same documents together. Further more, we can far easier merge files, without the need for many different in between versions of the same file. This improves both the speed, accuracy and communication within our projects. Within my personal repositories github helped to avoid that data would get lost, and made going back to older versions far easier. Further more, switching between different computers is also far easier with github, if the code is set up robust. "],["free-studying-period.html", "5 free studying period 5.1 planning and organisation to learn python 5.2 online python course 5.3 more basics 5.4 machine learning 5.5 code editors 5.6 conclusion", " 5 free studying period I have taken a course data science during the minor of my study. One of our assignments was to set our own learning goals for skills we want to obtain in the future. During my minor, I spend about 40 hours on getting started with learning these new skills. The aim was to get started, and to lay a foundation on which I could build further after I graduated. On this page, I will describe what I choose to do with this free learning period and how I spend my time on it. I got started by asking myself what I wanted to do and Where I wanted to be in ~2 years time. It was during this time that I realized I still have multiple things I want to learn and develop myself in within the life science field. These include: working in an ecological lab clinical lab work (like laboratory work in a hospital) working in a project with organoids Aside from these goals within the life science field, I also still had many personal goals. However, because these do not relate to this portfolio I did not add them here. But, it did rather quickly become clear to me that there is still a lot that I want to learn and do.This made it difficult for me to set my sight on a specific goal. Especially as Im not certain if I still want to continue working in the field of bioinformatics and data science in the future. Therefor, I decided to take a more general approach and get a wider few of whats all still possible for me with bioinformatics. This would help me to more easily adept, and learn specific skills later when Ive a more clear vision of what I want to do. To generally widen my view, I decided to learn python. This would be useful to more easily flow into jobs where python is a required programming language. Further more, python is a very useful language for software creation, which I would like to learn, to create my own software at some point in the future. This latter use is not necessarily needed for my job within life sciences, but, certainly could be an useful skill to obtain nevertheless. At the time I decided I wanted to learn how to program in python, I had no experience or knowledge of how to do so. Therefor, I had to start learning this new language from scratch, and start at the basics. 5.1 planning and organisation to learn python When trying to learn a new skill, a proper plan makes the learning process more structured and helps to see if your goals are achieved. In this free period I want to take the time to learn basic python from online tutorials on YouTube videos, and after that use this knowledge to create some own scripts, and to learn some basic machine learning. furthermore, I will try to use some different code editors. My general plan is: as I progress I will describing my learning process on this page and show what I learned (5 hours) get to know the basics by following an online class to learn python (20 hours) clear up any difficulties that I do not understand by: watching YouTube tutorials reading through some example python scripts and trying to recreate my own go on Reddit and read through existing posts or create my own machine learning in python (15 hours) watch YouTube tutorials, and write along with the videos to recreate the tutorial scripts apply the gained skills to create my own scripts from scratch and apply them to a dataset getting familiar with other code editors (5 hours): pychar Jupyter Notebook Visual Studio Code All python scripts I created can be found here All jupyter notebook files that I created can be found here 5.2 online python course I tried to learn the basics of python by following along with this online python course for beginners. However, after finishing 42% of the course I got stuck behind a pay wall, which I did not realize existed when I started the course. This course had to be payed with a credit card, which I do not have. Therefor, I could not finish the course. The skills I eventually practiced included: variables python data types lists if statements for loops Furthermore, I wrote a python script in jupyter notebook to analyze the prison break data set. This data set is a classic which is mentioned a lot in various python tutorials. The data set contains information about the amount of prison break attempts with helicopters. 5.3 more basics I did not want to start another 20 hour course (free), as this would simply take away to much of the time I had available to spend on this small project. Therefor, I watched this youtube video. in this video, I learned the following things: Variables Receiving Input Type Conversion Strings Arithmetic Operators Operator Precedence Comparison Operators Logical Operators If Statements While Loops Lists List Methods For Loops The range() Function Tuples This video went over quite a few basic things that I also learned in the (41%) course that I took. As I listened and watched the video, I came to realized that there are quite some similarities between python and r. I went on to practice with python, and made some basic python scripts, like: a basic calculator rock-paper-scissors game password generator a timer which can remind me to get up while coding 5.4 machine learning As I was learning the basics of python, I got more interested in machine learning and the possibilities that machine learning brings with it. Therefor, I watched this youtube video to learn more about machine learning (rather interesting I learn how to make a machine learn, aint it?). I got familiar with some of the most famous python machine learning libraries, including: numpy pandas matplotlib scikit-learning I got started with machine learning in pycharm, as this also the environment where I got started with learning python. However, the constant data inspection was rather messy and difficult in the terminal. Therefor, I rather quickly switched to using Jupyter notebook, as frequent data inspection is far easier in this program. The first form of machine learning I practiced with, was with a decision tree algorithm. This is an algorithm that is rather easy to understand and use. I used a decision tree algorithm to determine which drug patients received based on their Age, sex, blood pressure, Cholesterol, and Na_to_K. This is a rather simple method, that falls apart when using bigger more complex data sets. However, it is still sufficient to get some basic understandings. The results of this are shown in the jupyter notebook file located in the github directory of this portfolio. To further test my skills in importing, visualizing and analyzing data in python I analyzed my own YouTube watch history in another jupyter notebook file. Afterwards I tried to use a machine learning algorithm to analyze and estimate my watching behavior. This notebook can also be found in the github directory of this portfolio. 5.4.1 neuro networks A rather famous kind of machine learning that Ive heard a lot about are neuro networks. I did not understand how they worked, and therefor, I watched these videos: what is a neuronetwork? how neuronetworks learn what does backpropagation do? Backpropagation calculus In these videos, the creator really inspired me and suggested me to check out this book by Michael Nielsen introducing neural networks and deep learning. In this book, neuro networks get explained in more depth and an example with code is given. A neuro network was trained to recognize hand written digits with 96% accuracy.All code and data was open source and accessible from this github page.I also tried to type along to recreate the network. However, for this I was to inexperienced and the network did not work. Therefor, I did not include that into this repository. However, I will still continue to read this book in the future to get a proper understanding of neuronetworks and to get a better hang of it. 5.5 code editors Throughout the last year, I have for the most part used Rstudio, which has worked well for me so far. However, while learning python, I have seen people using many different code editors. And to further widen my few on what is all possible, I decided to try a few other code editors myself. Down bellow I describe which code editors I used and what my overall experience with them was. 5.5.1 pycharm I got started with learning python in PyCharm. This is an Integrated Development Environment (IDE). My overall experience with Pycharm was very positive. In Pycharm it is easy to create and manage projects and the build in vision control mechanisms work well. It is also possible to integrate git into pycharm. Further more The code editing is easy with good auto-indentation. The ease of use helped me to get comfortable with Python quickly. There are also many more advances options, and many customization options with packages, which makes this a great code editor for both new and experienced data annalists. A downside of Pycharm is that that the professional version with all features needs to be purchased. This makes this program less suitable within the idea of reproducible research and open science. Therefor, I decided to look for another (free) code editor or IDE. 5.5.2 visual studio code soon after getting started with watching python tutorials, I saw people using Visual Studio Code (VSC). This is an open-source text editor, and by far the most used code editor. Because this code editor is open-source, it is more suitable for reproducible research and open science. There are also many packages available due to the wide spread use of VSC. I noticed that, compared to pycharm, VSC is a more light weight code editor, that is easier on the computer but offers less processing power in return. The overall inter phase was easy to work with and offered a lot of customization. There is also a git plug in available, to add version control. This made VSC a pleasant code editor to work with and a great alternative to rstudio (while primarily working with python). 5.5.3 jupyter notebook I started working with jupyter notebook while learning some basics about machine learning. The main benefit of jupyter notebook is that, all results will be stored under the code chunks. These results will be saved and maintained until the code chunk is ran again. Therefor, code chunks only need to be run once and looking back at results becomes far easier. This makes it much easier to make small changes based upon earlier results. And further more, it felt convenient that unlike in R, where the data in the terminal disappears each time the r session is terminated, the data in jupyter notebook was maintained. Just like R studio and Visual studio code, jupyter notebook is free and helps the movement towards open science. 5.5.4 obsidian I came across obsidian while watching this youtube video in my spare time. Obsidian allows to edit R markdown documents and create networks between markdown files. Because I mainly focused on python In this period, I didnt go to much in depth into how obsidian works. But it seems like a program Id still want to try in the future. 5.6 conclusion As I worked with python in various different types of code editors, and than switched back to R studio to write this markdown file, I noticed how quickly I had grown unfamiliar with rstudio and that I started to confuse shortcuts and features from other programs. This showed me that, even though switching between different langues and programs may be relatively easy, it also has high value to be well familiar with the tools you use. Therefor, my entire idea to widen my few and get to know a bid about everything, might was not correct. For now I will stick to using R in R studio, and writing python in VSC. If in the future I Need to learn different programming languages, or get familiar with new code editors, I will make the switch when it is necessary. The time I spend into this project did help me to get started with learning python, I have been able use what I have learned to write my own scripts, and I already have some ideas of how I can continue with learning python in the future. Therefor, in my opinion I did finish this project properly. "],["project-liquid-biopsies.html", "6 project liquid biopsies 6.1 theoretical background 6.2 R project", " 6 project liquid biopsies Between May and June, 2022, I wrote R-code for the Princess Máxima Center within a project called liquid biopsies. I worked on this project together with 2 other students: Pedro de Bos and Thijmen van Brenk. On this page, I will provide: a theoretical back ground, to highlight the importance of the project liquid biopsies back ground information to what we tried to achieve in our R project a brief summary what the end product of our R project was 6.1 theoretical background Neuroblastoma is an embryonic malignancy that affects normal development of the adrenal medulla and para vertebral sympathetic ganglia in early childhood. Despite extensive studies to the molecular characteristics of human neuroblastomas, the initiation mechanisms and even its origin are still largely unknown. Maris (2010) However, neuroblastoma is the most common extracranial solid tumor in children. Further more, it has high recurrence rates and low survival rates. Therefore, early diagnosis, treatment response evaluation, and recurrence monitoring are of great significance for NB patients.  (2022) A further complication is that neuroblastoma exhibits genetic, morphological and clinical heterogeneity. This in term limits the efficacy of existing treatment. Therefor, Gaining detailed knowledge of the molecular signatures and genetic variations involved in the pathogenesis of neuroblastoma is necessary to develop safer and more effective treatments for neuroblastomas. Zafar et al. (2021) The wide genetic variation, also means that several different kinds of cell progresses can be effected. These include changes in the regulatory role in differentiation, apoptosis, cell proliferation, tumourigenesis, angiogenesis or metastasis of neuroblastoma. The mutations in different cell progresses may require different treatments, which can further complicate treatment when the genetic variation of the tumor cells is not well understood, Aygun (2018) The issues with neuroblastoma is that to properly monitor the progression of the tumor, samples need to be taken often. And tumor biopsies can be very invasive. This may lead to damage from the tumor biopsies, or to a lack of sampling which limits the effectiveness of the treatment. A possible solution, lays with liquid biopsies. Liquid biopsies, is a method of obtaining genomic material of tumors from blood samples. tumors secret DNA to the blood, which is called cell free DNA. This cell free DNA can be amplified, sequences and analysed. A study has shown that the proportion of tumor-derived DNA in cell free DNA was 42.5% (16.9%-55.9% across all samples). This could potentially mean sufficient sensitivity of liquid biopsy for neuroblastoma.@shiraiQuantitativeAssessmentCopy2022 6.2 R project At this moment, data sets of DNA from tumor biopsies and liquid biopsies of patients with neuoblastoma are compared manual at the Princes Maxima Center. This is very time consuming, and when working with time pressure from an increasing number of patients, can lead to human error. And within the view of open science, this method is not reproducible or robust. In our R project, we set out to solve this issue using the R-package called shinny. We used this package to build an interactive web database, which: automates the process of aligning DNA sets of samples allows for easy comparing and visualization of sequencing results introduces a reproducible and tidy work environment gives the possibility to easily save and share files requires a very limited amount of R-knowledge to set up (and comes with a manual) Our working partner at the princess Máxima Center was very satisfied with the end product we delivered. Though, because of an agreement of secrecy, I am not allowed to share any more details about this end product. "],["structured-query-language.html", "7 Structured Query Language 7.1 cleaning data 7.2 setting up a database 7.3 data visualization 7.4 joining graphs", " 7 Structured Query Language Structured Query Language (SQL), is a programming language which was creating for communicating with and editing databases. With the increasing usage of cloud storage and databases, the importance of SQL has become higher as the world got more automated. nearly all databases use SQL as their main programming language. Because of the high usage, learning SQL is very relevant. I currently have a basic knowledge of how to use SQL to create a database and to add, edit and remove data from databases. On this page, I will demonstrate what I can all do with SQL and build a SQL database. For this, I used two files that contained the number of dengue fever cases and flu cases from 2002 till 2015. Further more, some additional information about the countries was obtained from the gap minder data set, which is available as a R-package. I will make a short analysis of the data sets, in which the research aim is: to show the spread of flu and dengue fever and to test if there is any correlation between the spread of flu and dengue fever My hypothesis is that, because flu and dengue fever are different types of virusses, the spread will not be related. The general steps I took in this analysis are: Manual download of the Dengue Fever data set from the dsfb2 repository Manual download of the Flu Fever data set from the dsfb2 repository. install the gapminder r-package Clean and tidy the data Write the data away in the folder data Make a SQL database of the data using RPostgreSQL join the different data tables in the database download the joined table from the database Visualize the data in graphs if a correlation is suspected, perform descriptive statistics Conclusion 7.1 cleaning data After loading in the data, I inspected the data. The data from the gap minder data set is shown in table 7.1 The data from the dengue data set is shown in table 7.2 The data from the flu data set is shown in table 7.3 Figure 7.1: data from gap minder dataset Figure 7.2: data from dengue dataset Figure 7.3: data from flu dataset The gap minder data set was already in tidy data format, and therefor did not need to be edited. The dengue and flu data set did require editing to make the tables tidy. To make later comparisons of the different data sets easier, the format of the flu and dengue data sets were changed to match the format of the gap minder data set. The changes made were: Put all countries in one column, and the cases to one column split the column Date to three columns named Year, Month and Day correct data classes remove na data The new tidy dengue table is shown in table 7.4 The new tidy flu table is shown in table 7.5 The new tables were stored as a .csv file and as a .rds file in the data folder Figure 7.4: tidy data from dengue dataset Figure 7.5: tidy data from flu dataset 7.2 setting up a database 7.2.1 exporting data to DBeaver The program DBeaver was used to set up and maintain a database for this project. The new tidy data files were exporting to DBeaver using Rpostgres. This is further shown in the code chunk bellow. # the link to exporting files to DBeaver requires to fill in personal information such as a password, which is unique for all users. # this analysis is reproducible if you copy the code and insert your own local information. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;, user = &quot;postgres&quot;, password = &quot;fill in your own password&quot;) dbWriteTable(con, &quot;flu_tidy&quot;, flu_tidy) dbWriteTable(con, &quot;dengue_tidy&quot;, dengue_tidy) dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbListTables(con) dbDisconnect(con) After checking both in R using the dbListTables(con) function, and in DBeaver, it was confirmed that the data was imported properly. This can also be seen in the image bellow. inspecting if data was loaded into DBeaver 7.2.2 SQL script In DBeaver, the following script is written to further inspect the data. All used SQL scripts can also be found here. SELECT dengue_cases, country FROM dengue_tidy order BY dengue_cases asc SELECT flu_cases, country FROM flu_tidy order BY flu_cases asc SELECT population, country FROM gapminder order BY population desc The images bellow further show the data loaded into DBeaver: inspecting if dengue fever data was loaded into DBeaver inspecting if flu data was loaded into DBeaver inspecting if gapminder data was loaded into DBeaver After inspected the data in DBeaver, the data was also still inspected in R using the dplyr package. Bellow the data from the dengue, flu and gap minder data set are shown respectively. It becomes visible that, the data in these tables are the same as the data that was loaded into DBeaver. This confirms that the data was exported properly. ## # A tibble: 6,263 x 5 ## Year Month Day Country dengue_cases ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002 12 29 Bolivia 101 ## 2 2002 12 29 Brazil 73 ## 3 2002 12 29 India 62 ## 4 2002 12 29 Indonesia 101 ## 5 2002 12 29 Singapore 59 ## 6 2003 1 5 Bolivia 143 ## 7 2003 1 5 Brazil 98 ## 8 2003 1 5 India 47 ## 9 2003 1 5 Indonesia 39 ## 10 2003 1 5 Singapore 59 ## # ... with 6,253 more rows ## # A tibble: 17,266 x 5 ## Year Month Day Country flu_cases ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 2002 12 29 Brazil 174 ## 2 2002 12 29 Peru 329 ## 3 2003 1 5 Brazil 162 ## 4 2003 1 5 Peru 315 ## 5 2003 1 12 Brazil 174 ## 6 2003 1 12 Chile 1 ## 7 2003 1 12 Peru 314 ## 8 2003 1 19 Brazil 162 ## 9 2003 1 19 Chile 0 ## 10 2003 1 19 Peru 267 ## # ... with 17,256 more rows ## # A tibble: 10,545 x 9 ## country year infan~1 life_~2 ferti~3 popul~4 gdp conti~5 region ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Albania 1960 115. 62.9 6.19 1.64e6 NA Europe South~ ## 2 Algeria 1960 148. 47.5 7.65 1.11e7 1.38e10 Africa North~ ## 3 Angola 1960 208 36.0 7.32 5.27e6 NA Africa Middl~ ## 4 Antigua and Ba~ 1960 NA 63.0 4.43 5.47e4 NA Americ~ Carib~ ## 5 Argentina 1960 59.9 65.4 3.11 2.06e7 1.08e11 Americ~ South~ ## 6 Armenia 1960 NA 66.9 4.55 1.87e6 NA Asia Weste~ ## 7 Aruba 1960 NA 65.7 4.82 5.42e4 NA Americ~ Carib~ ## 8 Australia 1960 20.3 70.9 3.45 1.03e7 9.67e10 Oceania Austr~ ## 9 Austria 1960 37.3 68.8 2.7 7.07e6 5.24e10 Europe Weste~ ## 10 Azerbaijan 1960 NA 61.3 5.57 3.90e6 NA Asia Weste~ ## # ... with 10,535 more rows, and abbreviated variable names ## # 1: infant_mortality, 2: life_expectancy, 3: fertility, 4: population, ## # 5: continent The file from the gap minder data set still contained a lot of additional information from additional years, that was not included in the flu and dengue fever data set. Therefor, table 7.6 was created. In this table only the years 2002 - 2015 were selected. This new tidy data file from the gap minder data set was also exported to DBeaver using Rpostgres. Figure 7.6: tidy data from gapminder dataset 7.2.3 edditing data in DBeaver The flu and dengue fever data sets were joined together in DBeaver using SQL. All used SQL scripts can be found here. In total, three new tables were created: dengue_gapminder: this table combines the dengue fever cases with additional information from the gap minder r package flu_gapminder: this table combines the flu cases with additional information from the gap minder r package flu_dengue: this table combines the flu and gap minder cases 7.2.4 importing data from DBeaver after exporting the data from Rstudio to DBeaver, the data was imported back from DBeaver to Rstudio with the following script: # import the data into R after inspecting the data in DBeaver dengue_gapminder &lt;- dbReadTable(con, &quot;dengue_gapminder&quot;) flu_gapminder &lt;- dbReadTable(con, &quot;flu_gapminder&quot;) flu_dengue &lt;- dbReadTable(con, &quot;flu_dengue&quot;) 7.3 data visualization The data will now be visualized to check if the flu or dengue fever is more prevalent in certain areas. 7.3.1 flu data set The data from the flu data set is shown in figure 7.7. In this graph, for each country the amount of flu cases is shown on the Y axis and the years are shown on the X axis.A rather wide geological spread of data is visible, and reading accurate flu cases from this table is rather difficult. Therefor, the data is further summarized in table 7.8. In this data table the flu and gap minder data set are joined. When considering the number of flu cases globally as shown in figure 7.7 and table 7.8, it becomes visible that the highest occurrence of the flu is in South Africa, followed by Canada and the United States. To slightly less extend also countries like Mexico, Germany and Romania. The lowest flu occurrences on the other hand are found in Sweden, followed by Chile and New Zealand. When looking through the data in table 7.8, it becomes visible that there is no clear link between the amount of flu cases and the: infant mortality life expectancy fertility gdp further more, the geological spread is also rather wide. A general trend that does appear to be visible is that: South Africa has by far the most amount of flu cases, however, this is a continent with a very large population and therefor directly comparing the amount of flu cases between south Africa and other far smaller countries/continents would not statistically relevant. To properly compare the difference the amount of flu cases should be expressed as a fraction of the population. However, due to missing data about the population size in new Zealand, south Africa and The united states, a full comparison of all the fractions is currently not possible. The percentage of the number of flu cases when compared to the population is expressed in figure 7.9. In this graph, the summarized average flu cases of each country is shown in the left. The percentage of the population is shown in the right graph. This graph shows a different spread, with countries with relatively low population sizes having a relatively higher percentage of flu cases, while countries with larger populations have a relatively speaking lower percentage of flu cases. Canada, the United States and Mexico appear to have far larger flu numbers than European countries. This could Possibly be explained by the difference in healthcare system, better vaccination rates, or climate in general. However, not enough meta data is available to concretely establish this hypothesis, and the collection and analysis of all this data falls beyond the scope of this research report. All in all, among most countries, the amount of flu cases does not appear to rise overall. After one or a few years with more flu cases, the amount of flu cases the next years appear to drop. The development of the global flu cases, and the specific kinds of flu that are currently present, will remain an important statistic to keep track of to keep flu vaccinations up to date for people with weakened immune systems. However, for people without a weakened immune system the amount of global flu cases does not appear to be concerning currently. Figure 7.7: first graph from flu dataset, sorted by country Figure 7.8: data table summarizing flu dataset Figure 7.9: joined graph from summary flu dataset, sorted by country 7.3.2 dengue fever data set The data from the dengue fever data set is shown in figure 7.10. This data has also been summarized in table 7.11, where the gap minder data set is joined to get some additional back ground information about the countries. When looking at figure 7.10, its visible that the highest number of dengue cases are seen in Venezuela, followed by Thailand and Singapore. While the lowest cases are seen in Bolivia, Argentina and India. Further more in table 7.11 the percentage of dengue cases of the entire population of the country is calculated. This shows a different spread, with Singapore, Venezuela, and Bolivia having the highest Percentage of cases. While India, Indonesia and Brazil have the lowest number of dengue fever cases. India and Indonesia have relatively speaking very large population, and this could explain why the over all percentage of the population that gets dengue fever is so low. Dengue fever develops after infection with the Flaviviridae virus carried by mosquitoes. These mosquitoes are more prevalent in countries with warmer climates, which might explain the spread of occurrences that is visible within figure 7.10 There does not appear to be a clear link between the dengue cases and the: infant mortality life expectancy fertility gdp Figure 7.10: first graph from dengue dataset, sorted by country Figure 7.11: data table summarizing data from the dengue fever data set 7.4 joining graphs To be able to better compare the two occurrences of flu and dengue fever, the 2 plots will be joined together as is shown in figure 7.12. However, this comparison is not properly possible as the only country in which both flu and dengue cases have been reported is Argentina. To be able to better compare flu and dengue fever, more data would be needed. But no strong correlation between flu and dengue fever is suspected, as flu is an airborne disease, and dengue fever is a disease transmitted by a mosquito species. Figure 7.12: flu and dengue fever graphs joined together "],["rpackages.html", "8 rpackages", " 8 rpackages a rather big part of R, are the R packages that are available. These packages can add in many new features and customization. Further more, R packages make sharing code a lot easier. This supports the movement towards open source code and can save up a lot of time for everyone. To demonstrate I know how to build an R package, I created my own custom package called fiftyshadesofgrey. This R package adds a black and white theme to all graphs and plots. The pallets can further be customized with 3 base functions. Further more, this package includes 3 addition functions to quickly generate: table/data frame displaying the data types a scattered dot plot box plot line graph column graph bar graph correlation test between 2 variables inspecting the datatypes and distribution after importation is by far the most repetitive part of R code across my data analysis. The functions above make it easy and quick to roughly scan through newly imported data before a proper and tidy analysis will be performed and allow to remove a lot of repetitive code in my personal repositories. Besides the 3 customization functions and 3 analytical functions, A fourth analytical function is still in the making, which automatically detects the number of variables and performs the proper statistical comparison based on the distribution of the data. However, this function is currently not finished yet. The unfinished function is stored in my R folder of the repo of this portfolio under the name unfinished function_for_fityshadesofgrey. All instructions on how to use my costume package can be found at: https://github.com/Arthur1Timmermans/fiftyshadesofgrey Down bellow a short demonstration is given of fiftyshadesofgrey. Bare in mind that the package offers multiple color palettes and transparencies. ## id group x y ## 1 1 4 110 21.0 ## 2 2 4 110 21.0 ## 3 3 4 93 22.8 ## 4 4 3 110 21.4 ## 5 5 3 175 18.7 ## 6 6 3 105 18.1 ## 7 7 3 245 14.3 ## 8 8 4 62 24.4 ## 9 9 4 95 22.8 ## 10 10 4 123 19.2 ## 11 11 4 123 17.8 ## 12 12 3 180 16.4 ## 13 13 3 180 17.3 ## 14 14 3 180 15.2 ## 15 15 3 205 10.4 ## 16 16 3 215 10.4 ## 17 17 3 230 14.7 ## 18 18 4 66 32.4 ## 19 19 4 52 30.4 ## 20 20 4 65 33.9 ## 21 21 3 97 21.5 ## 22 22 3 150 15.5 ## 23 23 3 150 15.2 ## 24 24 3 245 13.3 ## 25 25 3 175 19.2 ## 26 26 4 66 27.3 ## 27 27 5 91 26.0 ## 28 28 5 113 30.4 ## 29 29 5 264 15.8 ## 30 30 5 175 19.7 ## 31 31 5 335 15.0 ## 32 32 4 109 21.4 ## [1] &quot;the p. values from x and y of the shapiro test are&quot; ## [1] 0.04880824 ## [1] 0.1228814 ## [1] &quot;the data was normaly distrubuted, a pearson correlation test was run to compare x and y and the P-value is&quot; ## [1] -0.7761684 "],["parameterization.html", "9 parameterization 9.1 introduction 9.2 data analysis 9.3 conclusion", " 9 parameterization parameterization is a good way to make code more robust and flexible. Furthermore, it also makes it easier for other people to change each other their codes without the risk of adding inconsistencies. In this chapter, I will make a paramererized report of the European Center for Disease Control (ECDC) COVID-19 case data. This data is available at the ECDC website. The raw data after downloading is shown in figure 9.1. In this data table it becomes visible that the data set contains data from various different kinds of countries, and data between 2020 and 2022. Therefor, Parameters were set up for: The country to which the report applies to The year that the reported data applies to The period in months that the report applies to the introduction and results section bellow are loaded in with the parameters. Changing the parameters will automatically change these sections. The conclusion is written without the usage of parameters, as the conclusion is the interpretation of the statistical analysis. And setting up a robust automatic rendering conclusion would require a lot of r-code and would go beyond the scope of this document. Further more, the interpretation of the graphs, is also not done automatically, as this would require an advanced algorithm which goes beyond the scope of this file. However, the interpretation of the statistical tests is done automatically. The underlying code can be viewed in my personal repo. Figure 9.1: raw data with covid cases 9.1 introduction In this report, Austria, 2021, month 1 - 12, were ultimately chosen as parameter conditions. The new data table is shown in table 9.2. This data table also has a new column called percentage_deaths, which expresses the COVID-19 deaths as a percentage of the total amount of cases. The further aim of this report is to analyse the correlation between the time of the year (the months), the COVID-19 cases and the COVID-19 related deaths in Austria in 2021. Figure 9.2: data with covid cases after filtering on params 9.2 data analysis The new COVID-19 cases in Austria in 2021 are mapped in figure 9.3. In this graph, the y axis shows the time in months and the x axis shows the amount of new COVID-19 cases. It is visible that by far the largest peak in COVID-19 cases is measured between October and December in 2021. This would correspond with the flu/cold season. Figure 9.3: graph visualizing the amount of new covid 19 cases The deaths caused by COVID-19 in Austria in 2021 are mapped in figure 9.4. In this graph, the y axis shows the time in months and the x axis shows the amount of people who died as a result of COVID-19. Within this plot, a line somewhat similar to the COVID-19 cases in figure 9.3 can be seen, with a large spike in COVID-19 deaths between October and December 2021. Figure 9.4: graph visualizing the amount of covid 19 deaths To further visualize the similarity between figure 9.3 and figure 9.4, the graphs are also combined using the plot_grind function. This created figure 9.5, in which a correlation between the COVID-19 cases and the COVID-19 deaths appears to be present. Figure 9.5: combining the graphs of covid 19 cases and death The normality of the COVID-19 cases and deaths in Austria in 2021 was tested with a Shapiro Wilk test. This resulted in a p value of 5.8935071^{-26} and 5.9776348^{-14} respectively. These P-values are lower than 0,05. Therefor, the null hypothesis that the data is normally distributed is discarded, and the alternative hypothesis is accepted that the data is not normally distributed. To further test the correlation of the COVID-19 cases and COVID-19 related deaths in Austria in 2021, a non-paramatric had to be used because the data was not normally distributed. The COVID-19 cases were directly tested against the COVID-19 related deaths with a spearman correlation test. The correlation coefficient of the spearman correlation test was 0.6776374 This correlation coefficient can be interpreted as that the COVID-19 cases and COVID-19 related deaths are moderately correlated. When the percentage of COVID-19 cases that died in Austria in 2021 are put into a graph, this would show the figure in graph 9.6. In this graph the percentage of the COVID-19 cases who died as a result of COVID-19 are shown on the y axis and the progression of the time in months is shown on the x-axis. This graph appears to follow the earlier seen trends in 9.3 and 9.4 far less. The percentage of people who got COVID-19 and died as a results of this, appears to generally get lower as 2021 progresses. Figure 9.6: graph showing the percentage of covid19 cases that died To further test if progression of 2021 in Austria is correlated to the percentage of COVID-19 cases that died as a result of COVID-19, a spearman correlation test was run because the data was not normally distributed. The correlation coefficient of the spearman correlation test was -0.6222432 This correlation coefficient can be interpreted as that the progression of the year and the percentage of COVID-19 cases who died as a result of COVID-19 are moderately negativly correlated. 9.3 conclusion In conclusion, the COVID-19 cases and deaths appear to be moderately correlated. However, a moderate negative correlation also exists between the percentage of COVID-19 cases that died as a result of COVID-19 and the progression of the year 2021. This can possibly be explained by the availability and effectiveness of vaccinations against COVID-19. Further more, group immunity may rise naturally as well. The rise of group immunity can decrease the severity of the cases, and therefor decrease the amount of people who died as a result of COVID-19. This lowers the deaths for 100 cases at the end of 2021, when compared to the start of 2021. This would also explain why the COVID-19 cases and COVID-19 deaths are only moderately correlated and not strongly (linear) correlated. "],["refrences.html", "10 refrences", " 10 refrences The links to all used sources: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
